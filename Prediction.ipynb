{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de36a35",
   "metadata": {},
   "source": [
    "## Prediction für Test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04556e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Apply-Pipeline für gespeicherte Modelle (XGBoost u.a.) ====\n",
    "# Voraussetzungen (liegen bereits vor):\n",
    "#   - xgb_final.pkl                (sklearn-Wrapper)      ODER\n",
    "#   - xgb_final_booster.json       (reiner Booster Fallback)\n",
    "#   - xgb_columns.txt              (OHE-Spaltenliste aus dem Training)\n",
    "#   - xgb_meta.json                (use_log, use_smearing, smear, optional: rare_levels_map)\n",
    "#\n",
    "# test.csv: enthält neue Daten; wenn 'loss' vorhanden ist -> werden Metriken berechnet.\n",
    "#\n",
    "# Nutzung: einfach am Ende run_apply(\"test.csv\")\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ---------------------- Pfade & Konstanten ----------------------\n",
    "ARTIFACTS_DIR = Path(\".\")  # ggf. anpassen\n",
    "MODEL_PKL      = ARTIFACTS_DIR / \"xgb_final.pkl\"\n",
    "MODEL_BOOSTER  = ARTIFACTS_DIR / \"xgb_final_booster.json\"\n",
    "COLUMNS_TXT    = ARTIFACTS_DIR / \"xgb_columns.txt\"\n",
    "META_JSON      = ARTIFACTS_DIR / \"xgb_meta.json\"\n",
    "\n",
    "ID_COL = \"id\"\n",
    "TARGET_COL = \"loss\"\n",
    "\n",
    "# explizite Drops\n",
    "DROP_ALWAYS = [\"cont12\"]  # zusätzlich zur ID\n",
    "DROP_CATS   = [\"cat70\", \"cat15\", \"cat22\", \"cat64\", \"cat62\", \"cat63\", \"cat68\", \"cat55\", \"cat56\"]\n",
    "\n",
    "# RARE-Kompressions-Liste\n",
    "RARE_COLS = [\"cat20\",\"cat35\",\"cat58\",\"cat48\",\"cat59\",\"cat69\",\"cat21\",\"cat60\",\"cat34\",\"cat67\",\n",
    "             \"cat47\",\"cat61\",\"cat77\",\"cat46\",\"cat33\",\"cat18\",\"cat32\",\"cat51\",\"cat17\",\"cat42\",\"cat78\"]\n",
    "\n",
    "# Fallback-Schwelle, falls kein Mapping im Meta vorhanden\n",
    "RARE_MIN_COUNT = 20\n",
    "RARE_LABEL = \"RARE\"\n",
    "\n",
    "# ---------------------- Hilfsfunktionen ----------------------\n",
    "def load_meta(meta_path=META_JSON):\n",
    "    meta = {}\n",
    "    if Path(meta_path).exists():\n",
    "        with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "    # Defaults\n",
    "    meta.setdefault(\"use_log\", False)\n",
    "    meta.setdefault(\"use_smearing\", False)\n",
    "    meta.setdefault(\"smear\", 1.0)\n",
    "    # optional: {\"rare_levels_map\": {\"cat20\": [\"A\",\"B\",\"RARE\"], ...}}\n",
    "    return meta\n",
    "\n",
    "def load_columns(columns_path=COLUMNS_TXT):\n",
    "    with open(columns_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cols = [ln.strip() for ln in f if ln.strip()]\n",
    "    if not cols:\n",
    "        raise RuntimeError(\"xgb_columns.txt ist leer – ohne Spaltenliste kann nicht ausgerichtet werden.\")\n",
    "    return cols\n",
    "\n",
    "def compress_rare_levels(df, cols, rare_map=None, min_count=RARE_MIN_COUNT, rare_label=RARE_LABEL):\n",
    "    \"\"\"Komprimiere seltene Levels deterministisch:\n",
    "       - Wenn rare_map vorhanden (aus Training): alles außerhalb der erlaubten Menge -> RARE\n",
    "       - Sonst: zähle im aktuellen df und komprimiere Levels mit Count < min_count zu RARE\n",
    "    \"\"\"\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        if rare_map and c in rare_map:\n",
    "            allowed = set(rare_map[c])\n",
    "            df[c] = df[c].where(df[c].isin(allowed), rare_label)\n",
    "        else:\n",
    "            vc = df[c].value_counts(dropna=False)\n",
    "            keep = set(vc[vc >= min_count].index)\n",
    "            df[c] = df[c].where(df[c].isin(keep), rare_label)\n",
    "    return df\n",
    "\n",
    "def preprocess_unseen(df_raw, columns_txt=COLUMNS_TXT, meta=None):\n",
    "    \"\"\"Repliziere das Training-Preprocessing:\n",
    "       1) ID separieren\n",
    "       2) feste Drops\n",
    "       3) Rare-Kompression (Mapping aus meta, sonst Fallback)\n",
    "       4) One-Hot-Encoding\n",
    "       5) Spalten auf Trainings-OHE ausrichten\n",
    "    \"\"\"\n",
    "    if meta is None:\n",
    "        meta = load_meta()\n",
    "\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # 1) Ziel & ID separieren\n",
    "    y = None\n",
    "    if TARGET_COL in df.columns:\n",
    "        y = df[TARGET_COL].copy()\n",
    "        df = df.drop(columns=[TARGET_COL])\n",
    "    ids = df[ID_COL].copy() if ID_COL in df.columns else pd.Series(np.arange(len(df)), name=ID_COL)\n",
    "\n",
    "    # 2) Drop ID + feste Drops\n",
    "    drop_cols = [c for c in [ID_COL] if c in df.columns]\n",
    "    drop_cols += [c for c in DROP_ALWAYS if c in df.columns]\n",
    "    drop_cols += [c for c in DROP_CATS if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    # 3) Rare-Kompression (nur auf tatsächlich vorhandene Spalten)\n",
    "    rare_map = meta.get(\"rare_levels_map\", None)\n",
    "    df = compress_rare_levels(df, [c for c in RARE_COLS if c in df.columns],\n",
    "                              rare_map=rare_map, min_count=RARE_MIN_COUNT, rare_label=RARE_LABEL)\n",
    "\n",
    "    # 4) OHE\n",
    "    cat_cols = df.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
    "    df_ohe = pd.get_dummies(df, columns=cat_cols, drop_first=False, dtype=np.uint8)\n",
    "\n",
    "    # 5) Spaltenausrichtung\n",
    "    train_cols = load_columns(columns_txt)\n",
    "    X = df_ohe.reindex(columns=train_cols, fill_value=0)\n",
    "\n",
    "    return X, y, ids\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Lade sklearn-Wrapper, ansonsten Booster-Fallback.\"\"\"\n",
    "    model = None\n",
    "    meta = load_meta()\n",
    "    # 1) sklearn-Wrapper\n",
    "    if MODEL_PKL.exists():\n",
    "        try:\n",
    "            import joblib\n",
    "            model = joblib.load(MODEL_PKL)\n",
    "        except Exception:\n",
    "            with open(MODEL_PKL, \"rb\") as f:\n",
    "                model = pickle.load(f)\n",
    "        return (\"sklearn\", model, meta)\n",
    "    # 2) Booster-Fallback\n",
    "    if MODEL_BOOSTER.exists():\n",
    "        import xgboost as xgb\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(str(MODEL_BOOSTER))\n",
    "        return (\"booster\", booster, meta)\n",
    "    raise FileNotFoundError(\"Kein Modell gefunden: weder xgb_final.pkl noch xgb_final_booster.json vorhanden.\")\n",
    "\n",
    "def predict_with_loaded(model_kind, model_obj, X, meta):\n",
    "    \"\"\"Gibt y_pred auf Originalskala zurück (inkl. Log-Rücktransformation + Smearing).\"\"\"\n",
    "    use_log = bool(meta.get(\"use_log\", False))\n",
    "    use_smearing = bool(meta.get(\"use_smearing\", False))\n",
    "    smear = float(meta.get(\"smear\", 1.0))\n",
    "\n",
    "    if model_kind == \"sklearn\":\n",
    "        z = model_obj.predict(X)\n",
    "    else:\n",
    "        # Booster-Fallback\n",
    "        import xgboost as xgb\n",
    "        dmat = xgb.DMatrix(X)\n",
    "        z = model_obj.predict(dmat)\n",
    "\n",
    "    # Rücktransformation\n",
    "    if use_log:\n",
    "        y_pred = np.expm1(z)\n",
    "        if use_smearing:\n",
    "            y_pred = y_pred * smear\n",
    "    else:\n",
    "        y_pred = z\n",
    "    return y_pred\n",
    "\n",
    "def evaluate_if_possible(y_true, y_pred):\n",
    "    if y_true is None:\n",
    "        print(\"Hinweis: In test.csv ist keine Zielspalte 'loss' vorhanden – gebe nur Vorhersagen aus.\")\n",
    "        return None\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    print(f\"MAE={mae:.4f} | RMSE={rmse:.2f} | R²={r2:.4f}\")\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "def run_apply(csv_path, save_preds=True, out_csv=\"predictions_xgb_test.csv\"):\n",
    "    \"\"\"Hauptfunktion: lädt test.csv, preprocess, lädt Modell, macht Vorhersagen, berechnet ggf. Metriken.\"\"\"\n",
    "    # 1) Daten laden\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # 2) Preprocess replizieren\n",
    "    meta = load_meta()\n",
    "    X, y, ids = preprocess_unseen(df, columns_txt=COLUMNS_TXT, meta=meta)\n",
    "\n",
    "    # 3) Modell laden\n",
    "    model_kind, model_obj, meta = load_model()\n",
    "\n",
    "    # 4) Vorhersage\n",
    "    y_pred = predict_with_loaded(model_kind, model_obj, X, meta)\n",
    "\n",
    "    # 5) Metriken (falls y vorhanden)\n",
    "    metrics = evaluate_if_possible(y, y_pred)\n",
    "\n",
    "    # 6) Speichern\n",
    "    if save_preds:\n",
    "        out_df = pd.DataFrame({ID_COL: ids.values, \"prediction\": y_pred})\n",
    "        out_df.to_csv(out_csv, index=False)\n",
    "        print(f\"Predictions gespeichert -> {out_csv}\")\n",
    "\n",
    "    return y_pred, metrics\n",
    "\n",
    "# --------- Beispielaufruf ----------\n",
    "y_pred, metrics = run_apply(\"data/test.csv\", save_preds=True, out_csv=\"predictions_xgb_test.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
