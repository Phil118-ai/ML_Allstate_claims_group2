{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065c31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Import libraries and load data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load training dataset\n",
    "df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0cda2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'id' dropped. New shape: (188318, 131)\n"
     ]
    }
   ],
   "source": [
    "# ## Drop identifier column 'id'\n",
    "# Removes the 'id' column to avoid leakage / useless features in modeling.\n",
    "\n",
    "if 'id' in df.columns:\n",
    "    df = df.drop(columns=['id'])\n",
    "    print(\"'id' dropped. New shape:\", df.shape)\n",
    "elif df.index.name == 'id':\n",
    "    # If 'id' was set as the index, reset and then drop\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(\"'id' was index; reset_index done. New shape:\", df.shape)\n",
    "else:\n",
    "    print(\"No 'id' column found (and not used as index).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc379b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (188318, 130) | y shape: (188318,)\n"
     ]
    }
   ],
   "source": [
    "# ## X und y definieren (alle Features außer 'loss')\n",
    "target = \"loss\"\n",
    "assert target in df.columns, \"Spalte 'loss' fehlt in df.\"\n",
    "\n",
    "y = df[target]                  # Zielvariable\n",
    "X = df.drop(columns=[target])   # alle übrigen Spalten als Features\n",
    "\n",
    "print(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb999f6b",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97f8646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fdc814b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-variance features (from train): []\n",
      "Shapes -> X_train: (150654, 130) | X_test: (37664, 130)\n"
     ]
    }
   ],
   "source": [
    "# ## 5. Zero variance features (learn on X_train, apply to both train/test)\n",
    "\n",
    "# Identify zero-variance columns on TRAIN only\n",
    "zero_var = [col for col in X_train.columns if X_train[col].nunique(dropna=False) <= 1]\n",
    "print(\"Zero-variance features (from train):\", zero_var)\n",
    "\n",
    "# Drop the same columns from TRAIN (and TEST if available)\n",
    "X_train = X_train.drop(columns=zero_var, errors=\"ignore\")\n",
    "if \"X_test\" in globals():\n",
    "    X_test = X_test.drop(columns=zero_var, errors=\"ignore\")\n",
    "\n",
    "print(\"Shapes -> X_train:\", X_train.shape, \"| X_test:\", X_test.shape if \"X_test\" in globals() else \"n/a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f257769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 6. Correlation matrix on X_train (numeric features only)\n",
    "\n",
    "# Compute correlation on TRAIN numerics only (avoids leakage)\n",
    "corr = X_train.corr(numeric_only=True)\n",
    "\n",
    "# Heatmap to visualize correlations\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", annot=False)\n",
    "plt.title(\"Correlation Heatmap (train only)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb556f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 6b. Correlation heatmap (upper triangle, annotate strong values) on X_train\n",
    "\n",
    "# Thresholded labels keep the plot readable; only |corr| >= 0.7 are shown\n",
    "threshold = 0.7\n",
    "corr = X_train.corr(numeric_only=True)\n",
    "\n",
    "# Build annotation array: show value if above threshold, else blank\n",
    "annot = corr.round(2).astype(str).mask(corr.abs() < threshold, \"\")\n",
    "\n",
    "# Mask lower triangle to avoid duplicates\n",
    "mask = np.tril(np.ones(corr.shape), k=0).astype(bool)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", mask=mask, annot=annot, fmt=\"\", annot_kws={\"size\":8})\n",
    "plt.title(f\"Correlation Heatmap (train only, upper triangle, labels if |corr| ≥ {threshold})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e788c15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "feature",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "VIF",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ced440a4-3a2a-45ee-b4fb-314faabb46ee",
       "rows": [
        [
         "11",
         "cont12",
         "639.9063370228408"
        ],
        [
         "10",
         "cont11",
         "620.6366364970131"
        ],
        [
         "5",
         "cont6",
         "135.87435561956798"
        ],
        [
         "0",
         "cont1",
         "94.47525283470998"
        ],
        [
         "8",
         "cont9",
         "88.55946441263941"
        ],
        [
         "9",
         "cont10",
         "57.308215110089606"
        ],
        [
         "12",
         "cont13",
         "35.21518499175908"
        ],
        [
         "6",
         "cont7",
         "30.5166791975888"
        ],
        [
         "7",
         "cont8",
         "14.498652870080349"
        ],
        [
         "2",
         "cont3",
         "14.08414560895006"
        ],
        [
         "3",
         "cont4",
         "11.03665953024167"
        ],
        [
         "1",
         "cont2",
         "10.708737526315353"
        ],
        [
         "4",
         "cont5",
         "7.299668086002464"
        ],
        [
         "13",
         "cont14",
         "5.583061274734428"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 14
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cont12</td>\n",
       "      <td>639.906337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cont11</td>\n",
       "      <td>620.636636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cont6</td>\n",
       "      <td>135.874356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cont1</td>\n",
       "      <td>94.475253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cont9</td>\n",
       "      <td>88.559464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cont10</td>\n",
       "      <td>57.308215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cont13</td>\n",
       "      <td>35.215185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cont7</td>\n",
       "      <td>30.516679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cont8</td>\n",
       "      <td>14.498653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cont3</td>\n",
       "      <td>14.084146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cont4</td>\n",
       "      <td>11.036660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cont2</td>\n",
       "      <td>10.708738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cont5</td>\n",
       "      <td>7.299668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cont14</td>\n",
       "      <td>5.583061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature         VIF\n",
       "11  cont12  639.906337\n",
       "10  cont11  620.636636\n",
       "5    cont6  135.874356\n",
       "0    cont1   94.475253\n",
       "8    cont9   88.559464\n",
       "9   cont10   57.308215\n",
       "12  cont13   35.215185\n",
       "6    cont7   30.516679\n",
       "7    cont8   14.498653\n",
       "2    cont3   14.084146\n",
       "3    cont4   11.036660\n",
       "1    cont2   10.708738\n",
       "4    cont5    7.299668\n",
       "13  cont14    5.583061"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 7. Detect multicollinearity (VIF) on X_train (train-only, no row drop)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Use only numeric columns from TRAIN\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Median-impute numerics for VIF calculation ONLY (avoids dropping rows)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_vif = pd.DataFrame(imp.fit_transform(X_train[num_cols]), columns=num_cols, index=X_train.index)\n",
    "\n",
    "# Compute VIF per feature\n",
    "vif_data = pd.DataFrame({\n",
    "    \"feature\": X_vif.columns,\n",
    "    \"VIF\": [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "}).sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "display(vif_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3297f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cont12    1.000000\n",
      "cont11    0.994369\n",
      "cont6     0.784550\n",
      "cont7     0.742557\n",
      "cont10    0.713273\n",
      "cont9     0.626101\n",
      "cont1     0.613556\n",
      "cont13    0.477018\n",
      "cont8     0.314358\n",
      "cont4     0.130324\n",
      "Name: cont12, dtype: float64\n",
      "cont10    0.713273\n",
      "cont9     0.626101\n",
      "cont1     0.613556\n",
      "cont13    0.477018\n",
      "cont8     0.314358\n",
      "cont4     0.130324\n",
      "cont2     0.105527\n",
      "cont14    0.049242\n",
      "cont3     0.005459\n",
      "cont5    -0.146701\n",
      "Name: cont12, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ## Find features collinear with cont12\n",
    "\n",
    "# Correlation of cont12 with all other numeric features\n",
    "corr_with_cont12 = X_train.corr(numeric_only=True)['cont12'].sort_values(ascending=False)\n",
    "\n",
    "# Show top correlations\n",
    "print(corr_with_cont12.head(10))\n",
    "print(corr_with_cont12.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3cb7fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of cont12 regressed on all other features: 0.9897799566958149\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "Z = X_train.drop(columns=['cont12']).select_dtypes(include=[np.number]).dropna()\n",
    "w = X_train['cont12'].dropna()\n",
    "\n",
    "model = LinearRegression().fit(Z, w)\n",
    "r2 = model.score(Z, w)\n",
    "\n",
    "print(\"R^2 of cont12 regressed on all other features:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d36e167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cont12 ↔ cont11 | correlation = 0.994\n"
     ]
    }
   ],
   "source": [
    "# ## Identify highly correlated feature pairs (threshold = 0.98)\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = X_train.corr(numeric_only=True).abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix (to avoid duplicates)\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation above threshold\n",
    "high_corr_pairs = [\n",
    "    (col, row, upper.loc[row, col])\n",
    "    for col in upper.columns\n",
    "    for row in upper.index\n",
    "    if upper.loc[row, col] > 0.98\n",
    "]\n",
    "\n",
    "# Display results\n",
    "for f1, f2, corr_val in high_corr_pairs:\n",
    "    print(f\"{f1} ↔ {f2} | correlation = {corr_val:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd8eaf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped from X_train: 1 col | from X_test: 1 col\n",
      "Remaining features in X_train: 129\n"
     ]
    }
   ],
   "source": [
    "# ## Drop 'cont12' from TRAIN and TEST\n",
    "\n",
    "cols_to_drop = [\"cont12\"]\n",
    "\n",
    "# Drop from X_train\n",
    "before_tr = X_train.shape[1]\n",
    "X_train = X_train.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "after_tr = X_train.shape[1]\n",
    "\n",
    "# Drop from X_test (if it exists)\n",
    "if \"X_test\" in globals():\n",
    "    before_te = X_test.shape[1]\n",
    "    X_test = X_test.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "    after_te = X_test.shape[1]\n",
    "    print(f\"Dropped from X_train: {before_tr - after_tr} col | from X_test: {before_te - after_te} col\")\n",
    "else:\n",
    "    print(f\"Dropped from X_train: {before_tr - after_tr} col | X_test not defined yet\")\n",
    "\n",
    "# Sanity checks\n",
    "assert \"cont12\" not in X_train.columns\n",
    "if \"X_test\" in globals():\n",
    "    assert \"cont12\" not in X_test.columns\n",
    "\n",
    "\n",
    "print(\"Remaining features in X_train:\", X_train.shape[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bc60a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "feature",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "VIF",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ef634f5e-adad-42f4-8a63-fe0adf163eb9",
       "rows": [
        [
         "5",
         "cont6",
         "134.85348488847805"
        ],
        [
         "0",
         "cont1",
         "94.45837532857948"
        ],
        [
         "8",
         "cont9",
         "88.37511948327592"
        ],
        [
         "9",
         "cont10",
         "57.30767946429296"
        ],
        [
         "11",
         "cont13",
         "35.0276253892154"
        ],
        [
         "6",
         "cont7",
         "30.466272895343792"
        ],
        [
         "10",
         "cont11",
         "26.372908385969524"
        ],
        [
         "7",
         "cont8",
         "14.410907229486723"
        ],
        [
         "2",
         "cont3",
         "14.084015569990349"
        ],
        [
         "3",
         "cont4",
         "11.01145399886944"
        ],
        [
         "1",
         "cont2",
         "10.662677017379053"
        ],
        [
         "4",
         "cont5",
         "7.291742270587278"
        ],
        [
         "12",
         "cont14",
         "5.5822878539531215"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 13
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cont6</td>\n",
       "      <td>134.853485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cont1</td>\n",
       "      <td>94.458375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cont9</td>\n",
       "      <td>88.375119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cont10</td>\n",
       "      <td>57.307679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cont13</td>\n",
       "      <td>35.027625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cont7</td>\n",
       "      <td>30.466273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cont11</td>\n",
       "      <td>26.372908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cont8</td>\n",
       "      <td>14.410907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cont3</td>\n",
       "      <td>14.084016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cont4</td>\n",
       "      <td>11.011454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cont2</td>\n",
       "      <td>10.662677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cont5</td>\n",
       "      <td>7.291742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cont14</td>\n",
       "      <td>5.582288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature         VIF\n",
       "5    cont6  134.853485\n",
       "0    cont1   94.458375\n",
       "8    cont9   88.375119\n",
       "9   cont10   57.307679\n",
       "11  cont13   35.027625\n",
       "6    cont7   30.466273\n",
       "10  cont11   26.372908\n",
       "7    cont8   14.410907\n",
       "2    cont3   14.084016\n",
       "3    cont4   11.011454\n",
       "1    cont2   10.662677\n",
       "4    cont5    7.291742\n",
       "12  cont14    5.582288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 7. Detect multicollinearity (VIF) on X_train (train-only, no row drop)\n",
    "\n",
    "# Use only numeric columns from TRAIN\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Median-impute numerics for VIF calculation ONLY (avoids dropping rows)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_vif = pd.DataFrame(imp.fit_transform(X_train[num_cols]), columns=num_cols, index=X_train.index)\n",
    "\n",
    "# Compute VIF per feature\n",
    "vif_data = pd.DataFrame({\n",
    "    \"feature\": X_vif.columns,\n",
    "    \"VIF\": [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "}).sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "display(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "059de031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cont6     1.000000\n",
      "cont10    0.883358\n",
      "cont13    0.814393\n",
      "cont9     0.796622\n",
      "cont11    0.773128\n",
      "cont1     0.757663\n",
      "cont7     0.659185\n",
      "cont8     0.435863\n",
      "cont4     0.219420\n",
      "cont14    0.040798\n",
      "Name: cont6, dtype: float64\n",
      "cont9     0.796622\n",
      "cont11    0.773128\n",
      "cont1     0.757663\n",
      "cont7     0.659185\n",
      "cont8     0.435863\n",
      "cont4     0.219420\n",
      "cont14    0.040798\n",
      "cont2     0.016578\n",
      "cont5    -0.149352\n",
      "cont3    -0.348852\n",
      "Name: cont6, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ## Find features collinear with cont6\n",
    "\n",
    "# Correlation of cont6 with all other numeric features\n",
    "corr_with_cont6 = X_train.corr(numeric_only=True)['cont6'].sort_values(ascending=False)\n",
    "\n",
    "# Show top correlations\n",
    "print(corr_with_cont6.head(10))\n",
    "print(corr_with_cont6.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc985f57",
   "metadata": {},
   "source": [
    "### we keep cont 6, since advanced models later can deal with these levels of collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b00fc",
   "metadata": {},
   "source": [
    "### Near constants for a lot of the categorial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6444535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat7: 97.58% same value\n",
      "cat14: 98.78% same value\n",
      "cat15: 99.98% same value\n",
      "cat16: 96.57% same value\n",
      "cat17: 99.30% same value\n",
      "cat18: 99.46% same value\n",
      "cat19: 99.04% same value\n",
      "cat20: 99.90% same value\n",
      "cat21: 99.78% same value\n",
      "cat22: 99.98% same value\n",
      "cat24: 96.62% same value\n",
      "cat28: 96.05% same value\n",
      "cat29: 98.02% same value\n",
      "cat30: 98.11% same value\n",
      "cat31: 97.15% same value\n",
      "cat32: 99.36% same value\n",
      "cat33: 99.50% same value\n",
      "cat34: 99.69% same value\n",
      "cat35: 99.89% same value\n",
      "cat39: 97.40% same value\n",
      "cat40: 95.67% same value\n",
      "cat41: 96.21% same value\n",
      "cat42: 99.10% same value\n",
      "cat43: 97.76% same value\n",
      "cat45: 97.72% same value\n",
      "cat46: 99.53% same value\n",
      "cat47: 99.62% same value\n",
      "cat48: 99.86% same value\n",
      "cat49: 95.15% same value\n",
      "cat51: 99.34% same value\n",
      "cat52: 95.35% same value\n",
      "cat54: 97.57% same value\n",
      "cat55: 99.92% same value\n",
      "cat56: 99.90% same value\n",
      "cat57: 98.39% same value\n",
      "cat58: 99.88% same value\n",
      "cat59: 99.84% same value\n",
      "cat60: 99.77% same value\n",
      "cat61: 99.62% same value\n",
      "cat62: 99.98% same value\n",
      "cat63: 99.96% same value\n",
      "cat64: 99.98% same value\n",
      "cat65: 98.79% same value\n",
      "cat66: 95.58% same value\n",
      "cat67: 99.62% same value\n",
      "cat68: 99.93% same value\n",
      "cat69: 99.83% same value\n",
      "cat70: 99.99% same value\n",
      "cat74: 98.09% same value\n",
      "cat76: 96.29% same value\n",
      "cat77: 99.56% same value\n",
      "cat78: 99.04% same value\n",
      "cat85: 98.77% same value\n",
      "cat89: 97.58% same value\n"
     ]
    }
   ],
   "source": [
    "# ## 8. Detect near-zero variance categorical features (X_train)\n",
    "\n",
    "# For categorical columns: ratio of most frequent value (train-only)\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    vc = X_train[col].value_counts(normalize=True, dropna=True)\n",
    "    if len(vc) == 0:\n",
    "        continue  # skip columns that are entirely NaN\n",
    "    top_freq = vc.iloc[0]\n",
    "    if top_freq > 0.95:  # 95%+ same value → low information\n",
    "        print(f\"{col}: {top_freq:.2%} same value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4634f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUST_DROP (≥ 0.999): 9\n",
      "RARE_BUCKET ([0.98, 0.999)): 29\n",
      "REVIEW ([0.95, 0.98)): 16\n",
      "\n",
      "Examples — MUST_DROP: ['cat70', 'cat15', 'cat22', 'cat64', 'cat62', 'cat63', 'cat68', 'cat55', 'cat56']\n",
      "Examples — RARE_BUCKET: ['cat20', 'cat35', 'cat58', 'cat48', 'cat59', 'cat69', 'cat21', 'cat60', 'cat34', 'cat67']\n",
      "Examples — REVIEW: ['cat43', 'cat45', 'cat7', 'cat89', 'cat54', 'cat39', 'cat31', 'cat24', 'cat16', 'cat76']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "top_freq",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bucket",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "6af6f3bd-047f-4286-94a1-0578d2bc0157",
       "rows": [
        [
         "cat70",
         "0.9998672454763896",
         "must_drop"
        ],
        [
         "cat15",
         "0.9998141436669454",
         "must_drop"
        ],
        [
         "cat22",
         "0.9997809550360428",
         "must_drop"
        ],
        [
         "cat64",
         "0.9997610418575013",
         "must_drop"
        ],
        [
         "cat62",
         "0.9997610418575013",
         "must_drop"
        ],
        [
         "cat63",
         "0.9995950987029883",
         "must_drop"
        ],
        [
         "cat68",
         "0.9992764878463234",
         "must_drop"
        ],
        [
         "cat55",
         "0.9992034728583377",
         "must_drop"
        ],
        [
         "cat56",
         "0.9990441674300051",
         "must_drop"
        ],
        [
         "cat20",
         "0.9989711524420195",
         "rare_bucket"
        ],
        [
         "cat35",
         "0.9988583110969507",
         "rare_bucket"
        ],
        [
         "cat58",
         "0.9987587452042428",
         "rare_bucket"
        ],
        [
         "cat48",
         "0.9985596134188273",
         "rare_bucket"
        ],
        [
         "cat59",
         "0.9984467720737584",
         "rare_bucket"
        ],
        [
         "cat69",
         "0.998314017550148",
         "rare_bucket"
        ],
        [
         "cat21",
         "0.9977829994557065",
         "rare_bucket"
        ],
        [
         "cat60",
         "0.9976502449320961",
         "rare_bucket"
        ],
        [
         "cat34",
         "0.9968669932427947",
         "rare_bucket"
        ],
        [
         "cat67",
         "0.9962430469818259",
         "rare_bucket"
        ],
        [
         "cat47",
         "0.9962430469818259",
         "rare_bucket"
        ],
        [
         "cat61",
         "0.9961899451723818",
         "rare_bucket"
        ],
        [
         "cat77",
         "0.9956257384470376",
         "rare_bucket"
        ],
        [
         "cat46",
         "0.9953336784950947",
         "rare_bucket"
        ],
        [
         "cat33",
         "0.9949818790075272",
         "rare_bucket"
        ],
        [
         "cat18",
         "0.9946300795199596",
         "rare_bucket"
        ],
        [
         "cat32",
         "0.9936012319619791",
         "rare_bucket"
        ],
        [
         "cat51",
         "0.9934021001765635",
         "rare_bucket"
        ],
        [
         "cat17",
         "0.9930104743319128",
         "rare_bucket"
        ],
        [
         "cat42",
         "0.9909859678468543",
         "rare_bucket"
        ],
        [
         "cat78",
         "0.9904151233953297",
         "rare_bucket"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 30
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_freq</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat70</th>\n",
       "      <td>0.999867</td>\n",
       "      <td>must_drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat15</th>\n",
       "      <td>0.999814</td>\n",
       "      <td>must_drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat22</th>\n",
       "      <td>0.999781</td>\n",
       "      <td>must_drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat64</th>\n",
       "      <td>0.999761</td>\n",
       "      <td>must_drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat62</th>\n",
       "      <td>0.999761</td>\n",
       "      <td>must_drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat63</th>\n",
       "      <td>0.999595</td>\n",
       "      <td>must_drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat68</th>\n",
       "      <td>0.999276</td>\n",
       "      <td>must_drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat55</th>\n",
       "      <td>0.999203</td>\n",
       "      <td>must_drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat56</th>\n",
       "      <td>0.999044</td>\n",
       "      <td>must_drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat20</th>\n",
       "      <td>0.998971</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat35</th>\n",
       "      <td>0.998858</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat58</th>\n",
       "      <td>0.998759</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat48</th>\n",
       "      <td>0.998560</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat59</th>\n",
       "      <td>0.998447</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat69</th>\n",
       "      <td>0.998314</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat21</th>\n",
       "      <td>0.997783</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat60</th>\n",
       "      <td>0.997650</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat34</th>\n",
       "      <td>0.996867</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat67</th>\n",
       "      <td>0.996243</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat47</th>\n",
       "      <td>0.996243</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat61</th>\n",
       "      <td>0.996190</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat77</th>\n",
       "      <td>0.995626</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat46</th>\n",
       "      <td>0.995334</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat33</th>\n",
       "      <td>0.994982</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat18</th>\n",
       "      <td>0.994630</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat32</th>\n",
       "      <td>0.993601</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat51</th>\n",
       "      <td>0.993402</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat17</th>\n",
       "      <td>0.993010</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat42</th>\n",
       "      <td>0.990986</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat78</th>\n",
       "      <td>0.990415</td>\n",
       "      <td>rare_bucket</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       top_freq       bucket\n",
       "cat70  0.999867    must_drop\n",
       "cat15  0.999814    must_drop\n",
       "cat22  0.999781    must_drop\n",
       "cat64  0.999761    must_drop\n",
       "cat62  0.999761    must_drop\n",
       "cat63  0.999595    must_drop\n",
       "cat68  0.999276    must_drop\n",
       "cat55  0.999203    must_drop\n",
       "cat56  0.999044    must_drop\n",
       "cat20  0.998971  rare_bucket\n",
       "cat35  0.998858  rare_bucket\n",
       "cat58  0.998759  rare_bucket\n",
       "cat48  0.998560  rare_bucket\n",
       "cat59  0.998447  rare_bucket\n",
       "cat69  0.998314  rare_bucket\n",
       "cat21  0.997783  rare_bucket\n",
       "cat60  0.997650  rare_bucket\n",
       "cat34  0.996867  rare_bucket\n",
       "cat67  0.996243  rare_bucket\n",
       "cat47  0.996243  rare_bucket\n",
       "cat61  0.996190  rare_bucket\n",
       "cat77  0.995626  rare_bucket\n",
       "cat46  0.995334  rare_bucket\n",
       "cat33  0.994982  rare_bucket\n",
       "cat18  0.994630  rare_bucket\n",
       "cat32  0.993601  rare_bucket\n",
       "cat51  0.993402  rare_bucket\n",
       "cat17  0.993010  rare_bucket\n",
       "cat42  0.990986  rare_bucket\n",
       "cat78  0.990415  rare_bucket"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 8a. Quantify dominance on X_train and bucket by action (train-only)\n",
    "# We split into: MUST_DROP (≈constant), RARE_BUCKET (very dominant), REVIEW (dominant but milder).\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cat_cols = X_train.select_dtypes(include=['object','category']).columns\n",
    "\n",
    "# Train-only top frequency per categorical column (includes NaN as a category)\n",
    "top_freq = (\n",
    "    X_train[cat_cols]\n",
    "    .apply(lambda s: s.value_counts(normalize=True, dropna=False).iloc[0] if s.notna().any() else np.nan)\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Thresholds aligned to your results:\n",
    "# - many around 99.8–100%  → MUST_DROP\n",
    "# - many around 98–99.8%   → RARE_BUCKET (compress rares)\n",
    "# - some around 95–98%     → REVIEW (decide later; often compress)\n",
    "MUST_DROP   = 0.999   # ≥ 99.9%\n",
    "RARE_BUCKET = 0.98    # ≥ 98% and < 99.9%\n",
    "REVIEW_LOW  = 0.95    # ≥ 95% and < 98%\n",
    "\n",
    "must_drop_cols   = top_freq[top_freq >= MUST_DROP].index.tolist()\n",
    "rare_bucket_cols = top_freq[(top_freq >= RARE_BUCKET) & (top_freq < MUST_DROP)].index.tolist()\n",
    "review_cols      = top_freq[(top_freq >= REVIEW_LOW)  & (top_freq < RARE_BUCKET)].index.tolist()\n",
    "\n",
    "print(f\"MUST_DROP (≥ {MUST_DROP:.3f}): {len(must_drop_cols)}\")\n",
    "print(f\"RARE_BUCKET ([{RARE_BUCKET:.2f}, {MUST_DROP:.3f})): {len(rare_bucket_cols)}\")\n",
    "print(f\"REVIEW ([{REVIEW_LOW:.2f}, {RARE_BUCKET:.2f})): {len(review_cols)}\")\n",
    "\n",
    "# (Optional) peek at a few names; full lists can be long\n",
    "print(\"\\nExamples — MUST_DROP:\", must_drop_cols[:10])\n",
    "print(\"Examples — RARE_BUCKET:\", rare_bucket_cols[:10])\n",
    "print(\"Examples — REVIEW:\", review_cols[:10])\n",
    "\n",
    "# Quick table to inspect\n",
    "summary = top_freq.to_frame('top_freq')\n",
    "summary['bucket'] = 'keep'\n",
    "summary.loc[must_drop_cols, 'bucket'] = 'must_drop'\n",
    "summary.loc[rare_bucket_cols, 'bucket'] = 'rare_bucket'\n",
    "summary.loc[review_cols, 'bucket'] = 'review'\n",
    "display(summary.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b1a098e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 9 constant-like categorical columns.\n",
      "New shapes -> X_train: (150654, 120) | X_test: (37664, 120)\n"
     ]
    }
   ],
   "source": [
    "# ## 8b. Drop truly constant-like categorical columns (X_train & X_test)\n",
    "\n",
    "# Drop from TRAIN\n",
    "X_train = X_train.drop(columns=must_drop_cols, errors=\"ignore\")\n",
    "\n",
    "# Drop from TEST (if present)\n",
    "if \"X_test\" in globals():\n",
    "    X_test = X_test.drop(columns=must_drop_cols, errors=\"ignore\")\n",
    "\n",
    "print(f\"Dropped {len(must_drop_cols)} constant-like categorical columns.\")\n",
    "print(\"New shapes -> X_train:\", X_train.shape, \"| X_test:\", X_test.shape if 'X_test' in globals() else \"n/a\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cab02b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed rare levels (<1%) to '__OTHER__' for 45 columns.\n"
     ]
    }
   ],
   "source": [
    "# ## 8c. Compress rare categories on TRAIN (learn) and apply to TRAIN/TEST\n",
    "# Columns: rare_bucket_cols + review_cols. Keep levels with >= 1% frequency; others -> \"__OTHER__\".\n",
    "\n",
    "\n",
    "MIN_FRAC = 0.01  # adjust if needed\n",
    "\n",
    "def _learn_keep_set(s: pd.Series, min_frac=MIN_FRAC):\n",
    "    counts = s.value_counts(normalize=True, dropna=True)\n",
    "    return set(counts[counts >= min_frac].index)\n",
    "\n",
    "def _compress_with_keep(s: pd.Series, keep, other=\"__OTHER__\"):\n",
    "    return s.where(s.isna() | s.isin(keep), other)\n",
    "\n",
    "# choose target columns present in X_train\n",
    "cols_to_compress = [c for c in (rare_bucket_cols + review_cols) if c in X_train.columns]\n",
    "\n",
    "# learn keep sets on TRAIN only\n",
    "keep_map = {col: _learn_keep_set(X_train[col], MIN_FRAC) for col in cols_to_compress}\n",
    "\n",
    "# apply to TRAIN\n",
    "for col, keep in keep_map.items():\n",
    "    X_train[col] = _compress_with_keep(X_train[col], keep)\n",
    "\n",
    "# apply to TEST with the SAME keep sets (no re-deciding on test)\n",
    "if \"X_test\" in globals():\n",
    "    for col, keep in keep_map.items():\n",
    "        if col in X_test.columns:\n",
    "            X_test[col] = _compress_with_keep(X_test[col], keep)\n",
    "\n",
    "print(f\"Compressed rare levels (<{MIN_FRAC:.0%}) to '__OTHER__' for {len(keep_map)} columns.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b11bb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_enc (drop='first'): (150654, 345)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phill\\Desktop\\Python_Basics\\ML_Allstate_claims_group2\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:202: UserWarning: Found unknown categories in columns [91, 99, 100, 104, 105, 106] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_enc  (drop='first'): (37664, 345)\n"
     ]
    }
   ],
   "source": [
    "# ## 8d (linear-friendly): One-hot with drop='first' (fit on train, transform test)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "X_num_tr = X_train[num_cols].copy()\n",
    "X_cat_tr = X_train[cat_cols].copy()\n",
    "\n",
    "# If your sklearn >= 1.1, you can still keep min_frequency; the *dropped* level is the first category in encoder.categories_\n",
    "try:\n",
    "    ohe_lin = OneHotEncoder(\n",
    "        drop='first',                 # <— THIS avoids the dummy trap\n",
    "        handle_unknown='ignore',\n",
    "        min_frequency=0.01,\n",
    "        sparse_output=False\n",
    "    )\n",
    "except TypeError:\n",
    "    ohe_lin = OneHotEncoder(\n",
    "        drop='first',\n",
    "        handle_unknown='ignore',\n",
    "        sparse=False\n",
    "    )\n",
    "\n",
    "# Fit on TRAIN only\n",
    "X_cat_tr_ohe = ohe_lin.fit_transform(X_cat_tr)\n",
    "ohe_cols = ohe_lin.get_feature_names_out(cat_cols)\n",
    "X_cat_tr_ohe = pd.DataFrame(X_cat_tr_ohe, columns=ohe_cols, index=X_train.index)\n",
    "\n",
    "X_train_enc = pd.concat([X_num_tr, X_cat_tr_ohe], axis=1)\n",
    "print(\"X_train_enc (drop='first'):\", X_train_enc.shape)\n",
    "\n",
    "# Transform TEST with the SAME encoder\n",
    "if \"X_test\" in globals():\n",
    "    X_num_te = X_test[num_cols].copy()\n",
    "    X_cat_te_ohe = ohe_lin.transform(X_test[cat_cols])\n",
    "    X_cat_te_ohe = pd.DataFrame(X_cat_te_ohe, columns=ohe_cols, index=X_test.index)\n",
    "    X_test_enc = pd.concat([X_num_te, X_cat_te_ohe], axis=1)\n",
    "    print(\"X_test_enc  (drop='first'):\", X_test_enc.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df791c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-variance cols (train, post-encode): 0\n",
      "Shapes -> X_train_enc: (150654, 345) | X_test_enc: (37664, 345)\n"
     ]
    }
   ],
   "source": [
    "# ## 8e. Drop zero-variance columns AFTER encoding (train-learned, apply to test)\n",
    "# Even after OHE some columns can be all-zero or all-one; detect on TRAIN only, then drop in both.\n",
    "\n",
    "# Identify zero-variance columns on TRAIN (post-encoding)\n",
    "zero_var_after = [c for c in X_train_enc.columns if X_train_enc[c].nunique(dropna=False) <= 1]\n",
    "print(f\"Zero-variance cols (train, post-encode): {len(zero_var_after)}\")\n",
    "\n",
    "# Drop from TRAIN\n",
    "X_train_enc = X_train_enc.drop(columns=zero_var_after, errors=\"ignore\")\n",
    "\n",
    "# Drop the same columns from TEST (if available)\n",
    "if \"X_test_enc\" in globals():\n",
    "    X_test_enc = X_test_enc.drop(columns=zero_var_after, errors=\"ignore\")\n",
    "\n",
    "print(\"Shapes -> X_train_enc:\", X_train_enc.shape, \"| X_test_enc:\", X_test_enc.shape if \"X_test_enc\" in globals() else \"n/a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c84afcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "column",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "top_freq",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "action",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5da65b2a-6a17-4af4-a383-09701bac17a1",
       "rows": [
        [
         "0",
         "cat70",
         "0.9998672454763896",
         "dropped_constant_like"
        ],
        [
         "1",
         "cat15",
         "0.9998141436669454",
         "dropped_constant_like"
        ],
        [
         "2",
         "cat22",
         "0.9997809550360428",
         "dropped_constant_like"
        ],
        [
         "3",
         "cat64",
         "0.9997610418575013",
         "dropped_constant_like"
        ],
        [
         "4",
         "cat62",
         "0.9997610418575013",
         "dropped_constant_like"
        ],
        [
         "5",
         "cat63",
         "0.9995950987029883",
         "dropped_constant_like"
        ],
        [
         "6",
         "cat68",
         "0.9992764878463234",
         "dropped_constant_like"
        ],
        [
         "7",
         "cat55",
         "0.9992034728583377",
         "dropped_constant_like"
        ],
        [
         "8",
         "cat56",
         "0.9990441674300051",
         "dropped_constant_like"
        ],
        [
         "9",
         "cat20",
         "0.9989711524420195",
         "compressed_rare_levels"
        ],
        [
         "10",
         "cat35",
         "0.9988583110969507",
         "compressed_rare_levels"
        ],
        [
         "11",
         "cat58",
         "0.9987587452042428",
         "compressed_rare_levels"
        ],
        [
         "12",
         "cat48",
         "0.9985596134188273",
         "compressed_rare_levels"
        ],
        [
         "13",
         "cat59",
         "0.9984467720737584",
         "compressed_rare_levels"
        ],
        [
         "14",
         "cat69",
         "0.998314017550148",
         "compressed_rare_levels"
        ],
        [
         "15",
         "cat21",
         "0.9977829994557065",
         "compressed_rare_levels"
        ],
        [
         "16",
         "cat60",
         "0.9976502449320961",
         "compressed_rare_levels"
        ],
        [
         "17",
         "cat34",
         "0.9968669932427947",
         "compressed_rare_levels"
        ],
        [
         "18",
         "cat67",
         "0.9962430469818259",
         "compressed_rare_levels"
        ],
        [
         "19",
         "cat47",
         "0.9962430469818259",
         "compressed_rare_levels"
        ],
        [
         "20",
         "cat61",
         "0.9961899451723818",
         "compressed_rare_levels"
        ],
        [
         "21",
         "cat77",
         "0.9956257384470376",
         "compressed_rare_levels"
        ],
        [
         "22",
         "cat46",
         "0.9953336784950947",
         "compressed_rare_levels"
        ],
        [
         "23",
         "cat33",
         "0.9949818790075272",
         "compressed_rare_levels"
        ],
        [
         "24",
         "cat18",
         "0.9946300795199596",
         "compressed_rare_levels"
        ],
        [
         "25",
         "cat32",
         "0.9936012319619791",
         "compressed_rare_levels"
        ],
        [
         "26",
         "cat51",
         "0.9934021001765635",
         "compressed_rare_levels"
        ],
        [
         "27",
         "cat17",
         "0.9930104743319128",
         "compressed_rare_levels"
        ],
        [
         "28",
         "cat42",
         "0.9909859678468543",
         "compressed_rare_levels"
        ],
        [
         "29",
         "cat78",
         "0.9904151233953297",
         "compressed_rare_levels"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 30
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>top_freq</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat70</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>dropped_constant_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat15</td>\n",
       "      <td>0.999814</td>\n",
       "      <td>dropped_constant_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat22</td>\n",
       "      <td>0.999781</td>\n",
       "      <td>dropped_constant_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat64</td>\n",
       "      <td>0.999761</td>\n",
       "      <td>dropped_constant_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat62</td>\n",
       "      <td>0.999761</td>\n",
       "      <td>dropped_constant_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cat63</td>\n",
       "      <td>0.999595</td>\n",
       "      <td>dropped_constant_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cat68</td>\n",
       "      <td>0.999276</td>\n",
       "      <td>dropped_constant_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cat55</td>\n",
       "      <td>0.999203</td>\n",
       "      <td>dropped_constant_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cat56</td>\n",
       "      <td>0.999044</td>\n",
       "      <td>dropped_constant_like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cat20</td>\n",
       "      <td>0.998971</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cat35</td>\n",
       "      <td>0.998858</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cat58</td>\n",
       "      <td>0.998759</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cat48</td>\n",
       "      <td>0.998560</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cat59</td>\n",
       "      <td>0.998447</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cat69</td>\n",
       "      <td>0.998314</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cat21</td>\n",
       "      <td>0.997783</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cat60</td>\n",
       "      <td>0.997650</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cat34</td>\n",
       "      <td>0.996867</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cat67</td>\n",
       "      <td>0.996243</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cat47</td>\n",
       "      <td>0.996243</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cat61</td>\n",
       "      <td>0.996190</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cat77</td>\n",
       "      <td>0.995626</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cat46</td>\n",
       "      <td>0.995334</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cat33</td>\n",
       "      <td>0.994982</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cat18</td>\n",
       "      <td>0.994630</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cat32</td>\n",
       "      <td>0.993601</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cat51</td>\n",
       "      <td>0.993402</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cat17</td>\n",
       "      <td>0.993010</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cat42</td>\n",
       "      <td>0.990986</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cat78</td>\n",
       "      <td>0.990415</td>\n",
       "      <td>compressed_rare_levels</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   column  top_freq                  action\n",
       "0   cat70  0.999867   dropped_constant_like\n",
       "1   cat15  0.999814   dropped_constant_like\n",
       "2   cat22  0.999781   dropped_constant_like\n",
       "3   cat64  0.999761   dropped_constant_like\n",
       "4   cat62  0.999761   dropped_constant_like\n",
       "5   cat63  0.999595   dropped_constant_like\n",
       "6   cat68  0.999276   dropped_constant_like\n",
       "7   cat55  0.999203   dropped_constant_like\n",
       "8   cat56  0.999044   dropped_constant_like\n",
       "9   cat20  0.998971  compressed_rare_levels\n",
       "10  cat35  0.998858  compressed_rare_levels\n",
       "11  cat58  0.998759  compressed_rare_levels\n",
       "12  cat48  0.998560  compressed_rare_levels\n",
       "13  cat59  0.998447  compressed_rare_levels\n",
       "14  cat69  0.998314  compressed_rare_levels\n",
       "15  cat21  0.997783  compressed_rare_levels\n",
       "16  cat60  0.997650  compressed_rare_levels\n",
       "17  cat34  0.996867  compressed_rare_levels\n",
       "18  cat67  0.996243  compressed_rare_levels\n",
       "19  cat47  0.996243  compressed_rare_levels\n",
       "20  cat61  0.996190  compressed_rare_levels\n",
       "21  cat77  0.995626  compressed_rare_levels\n",
       "22  cat46  0.995334  compressed_rare_levels\n",
       "23  cat33  0.994982  compressed_rare_levels\n",
       "24  cat18  0.994630  compressed_rare_levels\n",
       "25  cat32  0.993601  compressed_rare_levels\n",
       "26  cat51  0.993402  compressed_rare_levels\n",
       "27  cat17  0.993010  compressed_rare_levels\n",
       "28  cat42  0.990986  compressed_rare_levels\n",
       "29  cat78  0.990415  compressed_rare_levels"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## 8f. Audit: what changed on X_train (kept / dropped / compressed)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# If top_freq wasn't kept from step 8a, recompute it on X_train:\n",
    "try:\n",
    "    top_freq\n",
    "except NameError:\n",
    "    cat_cols = X_train.select_dtypes(include=['object','category']).columns\n",
    "    top_freq = (\n",
    "        X_train[cat_cols]\n",
    "        .apply(lambda s: s.value_counts(normalize=True, dropna=False).iloc[0] if s.notna().any() else float('nan'))\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "summary = []\n",
    "for col, tf in top_freq.items():\n",
    "    if 'must_drop_cols' in globals() and col in must_drop_cols:\n",
    "        status = \"dropped_constant_like\"\n",
    "    elif 'rare_bucket_cols' in globals() and 'review_cols' in globals() and (col in rare_bucket_cols or col in review_cols):\n",
    "        status = \"compressed_rare_levels\"\n",
    "    elif 'rare_bucket_cols' in globals() and col in rare_bucket_cols:\n",
    "        status = \"compressed_rare_levels\"\n",
    "    else:\n",
    "        status = \"kept\"\n",
    "    summary.append((col, tf, status))\n",
    "\n",
    "summary_df = pd.DataFrame(summary, columns=[\"column\",\"top_freq\",\"action\"]).sort_values(\"top_freq\", ascending=False)\n",
    "summary_df.head(30)  # inspect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b107b2",
   "metadata": {},
   "source": [
    "# Base line model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d28b1231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS -> R²=-19238817015440478208.0000 | RMSE=12529207753908.46 | MAE=75088083043.43\n"
     ]
    }
   ],
   "source": [
    "# ## Baseline: LinearRegression on already-split, encoded data\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train_enc, y_train)\n",
    "y_pred = ols.predict(X_test_enc)\n",
    "print(\"OLS -> R²={:.4f} | RMSE={:.2f} | MAE={:.2f}\".format(\n",
    "    r2_score(y_test, y_pred),\n",
    "    mean_squared_error(y_test, y_pred, squared=False),\n",
    "    mean_absolute_error(y_test, y_pred)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9020cb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS(top-150) -> R²=0.4912 | RMSE=2037.48 | MAE=1306.06\n",
      "Kept features: 150\n"
     ]
    }
   ],
   "source": [
    "# ## OLS with top-K features via univariate selection (on your encoded X)\n",
    "# Picks the K columns most correlated with y on TRAIN, then fits OLS on that subset.\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "K = 100  # try 50, 100, 150 ...\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=K)\n",
    "Xtr_sel = selector.fit_transform(X_train_enc, y_train)\n",
    "Xte_sel = selector.transform(X_test_enc)\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(Xtr_sel, y_train)\n",
    "y_pred = ols.predict(Xte_sel)\n",
    "\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"OLS(top-{K}) -> R²={r2:.4f} | RMSE={rmse:.2f} | MAE={mae:.2f}\")\n",
    "\n",
    "# (Optional) see which columns were kept\n",
    "kept_cols = X_train_enc.columns[selector.get_support()]\n",
    "print(\"Kept features:\", len(kept_cols))\n",
    "# display(kept_cols[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba179d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS(one=cat80_D) -> R²=0.2234 | RMSE=2517.28 | MAE=1644.04\n"
     ]
    }
   ],
   "source": [
    "# ## OLS with one very simple feature (sanity check)\n",
    "# Use just the single column with highest |corr| to y on TRAIN.\n",
    "\n",
    "if isinstance(X_train_enc, pd.DataFrame):\n",
    "    corr = X_train_enc.corrwith(pd.Series(y_train)).abs().dropna()\n",
    "    best_col = corr.idxmax()\n",
    "    Xtr_one = X_train_enc[[best_col]]\n",
    "    Xte_one = X_test_enc[[best_col]]\n",
    "else:\n",
    "    best_col = 0\n",
    "    Xtr_one = X_train_enc[:, [0]]\n",
    "    Xte_one = X_test_enc[:, [0]]\n",
    "\n",
    "ols1 = LinearRegression().fit(Xtr_one, y_train)\n",
    "yp1 = ols1.predict(Xte_one)\n",
    "\n",
    "print(f\"OLS(one={best_col}) -> R²={r2_score(y_test, yp1):.4f} | \"\n",
    "      f\"RMSE={mean_squared_error(y_test, yp1, squared=False):.2f} | \"\n",
    "      f\"MAE={mean_absolute_error(y_test, yp1):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db64c9e1",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130793be",
   "metadata": {},
   "source": [
    "### Ridge and K-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4de0166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha (RidgeCV): 31.622776601683793\n",
      "R²=0.5029 | RMSE=2013.89 | MAE=1293.95\n"
     ]
    }
   ],
   "source": [
    "# ## Fast(er) Ridge with RidgeCV + 3-fold CV\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)  # 3-fold is plenty for alpha tuning\n",
    "alphas = np.logspace(-2, 2, 9)  # shrink the grid\n",
    "\n",
    "ridge_cv = make_pipeline(\n",
    "    StandardScaler(),                          # helps convergence\n",
    "    RidgeCV(alphas=alphas, scoring='neg_mean_squared_error', cv=cv)\n",
    ")\n",
    "ridge_cv.fit(X_train_enc, y_train)\n",
    "\n",
    "y_pred = ridge_cv.predict(X_test_enc)\n",
    "print(\"Best alpha (RidgeCV):\", ridge_cv.named_steps['ridgecv'].alpha_)\n",
    "print(\"R²={:.4f} | RMSE={:.2f} | MAE={:.2f}\".format(\n",
    "    r2_score(y_test, y_pred),\n",
    "    mean_squared_error(y_test, y_pred, squared=False),\n",
    "    mean_absolute_error(y_test, y_pred)\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8ca87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Helper: metric print\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def report(name, y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f\"{name} -> R²={r2:.4f} | RMSE={rmse:.2f} | MAE={mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5b0dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoCV(top-K) alpha: 0.6579332246575679\n",
      "R²=0.4996 | RMSE=2020.65 | MAE=1297.11\n"
     ]
    }
   ],
   "source": [
    "# ## (Optional) Prefilter -> LassoCV faster & stabler (top-K univariate)\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "K = 200  # try 100–300\n",
    "sel = SelectKBest(f_regression, k=K).fit(X_train_enc, y_train)\n",
    "Xtr_k, Xte_k = sel.transform(X_train_enc), sel.transform(X_test_enc)\n",
    "\n",
    "lasso_k = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LassoCV(alphas=np.logspace(-1, 2, 12), cv=3, max_iter=30000, tol=1e-3, n_jobs=-1, random_state=42)\n",
    ")\n",
    "lasso_k.fit(Xtr_k, y_train)\n",
    "yp = lasso_k.predict(Xte_k)\n",
    "print(\"LassoCV(top-K) alpha:\", lasso_k.named_steps['lassocv'].alpha_)\n",
    "print(\"R²={:.4f} | RMSE={:.2f} | MAE={:.2f}\".format(\n",
    "    r2_score(y_test, yp), mean_squared_error(y_test, yp, squared=False), mean_absolute_error(y_test, yp)\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "526b9993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen by CV (subset): alpha=0.1874, l1_ratio=0.8\n",
      "ElasticNet (final) -> R²=0.5019 | RMSE=2016.00 | MAE=1291.62\n"
     ]
    }
   ],
   "source": [
    "# ## ElasticNet: fast CV on a subset, then refit on full data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 1) Subsample train for tuning (keeps it fast; adjust size)\n",
    "rng = np.random.RandomState(42)\n",
    "sub_n = min(50000, len(X_train_enc))   # try 50k; reduce to 30k if still slow\n",
    "idx = rng.choice(len(X_train_enc), size=sub_n, replace=False)\n",
    "\n",
    "Xtr_sub = X_train_enc.iloc[idx] if isinstance(X_train_enc, pd.DataFrame) else X_train_enc[idx]\n",
    "ytr_sub = y_train.iloc[idx] if hasattr(y_train, \"iloc\") else y_train[idx]\n",
    "\n",
    "# 2) CV on subset with stronger alphas + more iters (avoids non-convergence at tiny alphas)\n",
    "enet_cv = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    ElasticNetCV(\n",
    "        l1_ratio=[0.2, 0.5, 0.8],           # small, sane grid\n",
    "        alphas=np.logspace(-1, 2, 12),      # 0.1 ... 100 (avoid ultra-small alphas)\n",
    "        cv=3,\n",
    "        max_iter=30000,\n",
    "        tol=1e-3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    ")\n",
    "enet_cv.fit(Xtr_sub, ytr_sub)\n",
    "\n",
    "alpha  = enet_cv.named_steps[\"elasticnetcv\"].alpha_\n",
    "l1r    = enet_cv.named_steps[\"elasticnetcv\"].l1_ratio_\n",
    "print(f\"Chosen by CV (subset): alpha={alpha:.4g}, l1_ratio={l1r}\")\n",
    "\n",
    "# 3) Refit a plain ElasticNet on FULL training data with those hyperparams (one fit)\n",
    "enet_final = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    ElasticNet(alpha=alpha, l1_ratio=l1r, max_iter=30000, tol=1e-3, random_state=42)\n",
    ")\n",
    "enet_final.fit(X_train_enc, y_train)\n",
    "\n",
    "# 4) Test metrics\n",
    "y_pred = enet_final.predict(X_test_enc)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"ElasticNet (final) -> R²={r2:.4f} | RMSE={rmse:.2f} | MAE={mae:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0e56867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoosting -> R²=0.5718 | RMSE=1869.22 | MAE=1183.55\n"
     ]
    }
   ],
   "source": [
    "# ## 3) HistGradientBoostingRegressor — fast boosting baseline (no scaling needed)\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(\n",
    "    learning_rate=0.1, max_depth=None, max_iter=300, random_state=42,\n",
    "    early_stopping=True\n",
    ")\n",
    "hgb.fit(X_train_enc, y_train)\n",
    "y_pred = hgb.predict(X_test_enc)\n",
    "report(\"HistGradientBoosting\", y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46b1e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions among: ['cont2', 'cont7', 'cont3', 'cont11', 'cont6', 'cont4', 'cont8', 'cont10', 'cont14', 'cont9', 'cont1', 'cont5']\n",
      "RidgeCV + interactions(top-K numerics) -> R²=0.0427 | RMSE=2794.82 | MAE=1933.43\n"
     ]
    }
   ],
   "source": [
    "# ## 4) Cheap interactions: top-K numeric features + pairwise interactions + RidgeCV (3-fold)\n",
    "# Uses only the strongest K numeric features to keep it fast.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# pick top-K numeric features by |corr| with y (train-only, from PRE-ENCODING X_train)\n",
    "num_cols_orig = X_train.select_dtypes(include=[float, int]).columns\n",
    "corrs = X_train[num_cols_orig].corrwith(y_train).abs().dropna().sort_values(ascending=False)\n",
    "K = min(12, len(corrs))  # small K keeps it fast\n",
    "topK = corrs.head(K).index.tolist()\n",
    "print(\"Interactions among:\", topK)\n",
    "\n",
    "# build design from those K numerics only (ignore cats to stay fast)\n",
    "Xtr_num = X_train[topK].copy()\n",
    "Xte_num = X_test[topK].copy()\n",
    "\n",
    "inter_ridge = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\n",
    "    StandardScaler(),\n",
    "    RidgeCV(alphas=np.logspace(-2, 2, 9), cv=3, scoring='neg_mean_squared_error')\n",
    ")\n",
    "inter_ridge.fit(Xtr_num, y_train)\n",
    "y_pred = inter_ridge.predict(Xte_num)\n",
    "report(\"RidgeCV + interactions(top-K numerics)\", y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c6b69b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree(max_depth=8) -> R²=0.4617 | RMSE=2095.76 | MAE=1370.33\n"
     ]
    }
   ],
   "source": [
    "# ## 1) Decision Tree (quick depth sweep; will overfit if too deep)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "depths = [3, 5, 8, 12]\n",
    "\n",
    "best = None; best_rmse = float(\"inf\")\n",
    "for d in depths:\n",
    "    dt = DecisionTreeRegressor(max_depth=d, random_state=42)\n",
    "    # negative RMSE -> flip sign\n",
    "    rmse = -cross_val_score(dt, X_train_enc, y_train, cv=cv,\n",
    "                            scoring=\"neg_root_mean_squared_error\", n_jobs=-1).mean()\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse, best = rmse, dt\n",
    "\n",
    "best.fit(X_train_enc, y_train)\n",
    "pred = best.predict(X_test_enc)\n",
    "report(f\"DecisionTree(max_depth={best.get_params()['max_depth']})\", y_test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c604f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB R²: 0.5435454767936849\n",
      "RandomForest -> R²=0.5567 | RMSE=1901.96 | MAE=1217.17\n"
     ]
    }
   ],
   "source": [
    "# ## 2) Random Forest (use OOB for quick validation; no CV)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,            # small but solid\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=5,          # reduces overfitting, speeds up\n",
    "    n_jobs=-1, random_state=42,\n",
    "    oob_score=True, bootstrap=True\n",
    ")\n",
    "rf.fit(X_train_enc, y_train)\n",
    "print(\"OOB R²:\", rf.oob_score_)\n",
    "pred = rf.predict(X_test_enc)\n",
    "report(\"RandomForest\", y_test, pred)\n",
    "\n",
    "# (Optional) most important features\n",
    "# import pandas as pd\n",
    "# imp = pd.Series(rf.feature_importances_, index=getattr(X_train_enc, \"columns\", range(X_train_enc.shape[1])))\n",
    "# display(imp.sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba61db4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking(3×HGB → RidgeCV) -> R²=0.5756 | RMSE=1860.98 | MAE=1182.57\n"
     ]
    }
   ],
   "source": [
    "# ##HistGradientBoostingRegressor\n",
    "import numpy as np\n",
    "from sklearn.ensemble import StackingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "# 1) avoid pandas indexing surprises\n",
    "Xtr = np.asarray(X_train_enc)\n",
    "Xte = np.asarray(X_test_enc)\n",
    "\n",
    "# 2) CV splitter\n",
    "cv3 = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# 3) three diverse HGB base models\n",
    "base = HistGradientBoostingRegressor(\n",
    "    learning_rate=0.08, max_iter=400, early_stopping=True,\n",
    "    l2_regularization=1e-4\n",
    ")\n",
    "estimators = [\n",
    "    (\"hgb_dNone\", clone(base).set_params(max_depth=None,  random_state=41)),\n",
    "    (\"hgb_d6\",    clone(base).set_params(max_depth=6,     random_state=42)),\n",
    "    (\"hgb_d3\",    clone(base).set_params(max_depth=3,     random_state=43)),\n",
    "]\n",
    "\n",
    "meta = RidgeCV(alphas=np.logspace(-2, 2, 9), cv=3, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta,\n",
    "    cv=cv3,            # proper splitter\n",
    "    passthrough=False  # keep it simple; turn on later if needed\n",
    ")\n",
    "\n",
    "stack.fit(Xtr, y_train)\n",
    "y_pred = stack.predict(Xte)\n",
    "report(\"Stacking(3×HGB → RidgeCV)\", y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05e28846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTrees -> R²=0.5316 | RMSE=1954.89 | MAE=1233.94\n"
     ]
    }
   ],
   "source": [
    "# ## 4) Extra Trees (very fast, strong baseline)\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "et = ExtraTreesRegressor(\n",
    "    n_estimators=400, max_depth=None, min_samples_leaf=5,\n",
    "    n_jobs=-1, random_state=42\n",
    ")\n",
    "et.fit(X_train_enc, y_train)\n",
    "pred = et.predict(X_test_enc)\n",
    "report(\"ExtraTrees\", y_test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc3ed7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen blend weight w=0.25 (m1 share) on validation\n",
      "Blend(RandomForest, HistGB) -> R²=0.5766 | RMSE=1858.75 | MAE=1179.61\n"
     ]
    }
   ],
   "source": [
    "# ## 5) Simple blending (ensemble): average the best two models\n",
    "# Use a quick validation split from train to choose a weight, then refit each on full train and blend on test.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# pick two candidates to blend\n",
    "m1 = RandomForestRegressor(n_estimators=300, min_samples_leaf=5, n_jobs=-1, random_state=42, oob_score=False)\n",
    "m2 = HistGradientBoostingRegressor(learning_rate=0.1, max_iter=300, early_stopping=True, random_state=42)\n",
    "\n",
    "X_bl_tr, X_bl_va, y_bl_tr, y_bl_va = train_test_split(X_train_enc, y_train, test_size=0.2, random_state=42)\n",
    "m1.fit(X_bl_tr, y_bl_tr); p1 = m1.predict(X_bl_va)\n",
    "m2.fit(X_bl_tr, y_bl_tr); p2 = m2.predict(X_bl_va)\n",
    "\n",
    "# try a few weights\n",
    "weights = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "best_w, best_rmse = None, float(\"inf\")\n",
    "for w in weights:\n",
    "    blend = w*p1 + (1-w)*p2\n",
    "    rmse = mean_squared_error(y_bl_va, blend, squared=False)\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse, best_w = rmse, w\n",
    "print(f\"Chosen blend weight w={best_w} (m1 share) on validation\")\n",
    "\n",
    "# refit on full train and evaluate on test\n",
    "m1.fit(X_train_enc, y_train); t1 = m1.predict(X_test_enc)\n",
    "m2.fit(X_train_enc, y_train); t2 = m2.predict(X_test_enc)\n",
    "pred = best_w*t1 + (1-best_w)*t2\n",
    "report(\"Blend(RandomForest, HistGB)\", y_test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fda7419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed2c976351f492fa4b0526d517a35d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,088701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6070\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 684\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,644687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6080\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 685\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,133002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6078\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 684\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,157125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6076\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 684\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,183144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6076\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 684\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,168742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6075\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 684\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,169261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6078\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 684\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,154701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6072\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 684\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,166293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6073\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 684\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,169966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6073\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 684\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,234631 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6077\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 684\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,196728 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6069\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 684\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,375415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6071\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 685\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,029156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5075\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,028379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5069\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,030545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5059\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 176\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,051205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5054\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 176\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,041693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5055\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 176\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,035449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5060\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 176\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0,046056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5054\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 174\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,028096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5061\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 174\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,031072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5058\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 174\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,030835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5054\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 174\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,016650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5056\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 174\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,037931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5050\n",
      "[LightGBM] [Info] Number of data points in the train set: 105457, number of used features: 174\n",
      "[LightGBM] [Info] Start training from score 3041,688687\n",
      "66 ausgewählte Features:\n",
      "['cont8', 'cat81_D', 'cat37_B', 'cont4', 'cat50_B', 'cat84_C', 'cat3_B', 'cont7', 'cat1_B', 'cat26_B', 'cat9_B', 'cat100_I', 'cat4_B', 'cat12_B', 'cat73_B', 'cont13', 'cat112_AH', 'cont9', 'cat10_B', 'cat6_B', 'cat87_D', 'cat100_L', 'cat75_B', 'cat113_BM', 'cat5_B', 'cat113_AT', 'cat13_B', 'cat2_B', 'cat76_B', 'cat100_H', 'cont14', 'cat112_J', 'cat91_C', 'cat98_C', 'cat57_B', 'cat36_B', 'cat103_B', 'cat25_B', 'cat23_B', 'cat7_B', 'cont11', 'cat53_B', 'cat114_infrequent_sklearn', 'cat11_B', 'cat79_B', 'cat87_C', 'cat100_J', 'cat52_B', 'cont3', 'cat72_B', 'cat83_D', 'cat82_D', 'cont2', 'cat112_E', 'cat49_B', 'cat44_B', 'cat108_D', 'cat27_B', 'cat100_F', 'cat38_B', 'cat79_D', 'cont1', 'cont6', 'cat109_BI', 'cat100_G', 'cat80_D']\n"
     ]
    }
   ],
   "source": [
    "# --- Fix & Run BorutaShap (Regression, SHAP) ---\n",
    "\n",
    "import numpy as np\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "# Kompatibilitäts-Wrapper für SciPy (binomtest vs. binom_test) + Integer-Cast\n",
    "try:\n",
    "    from scipy.stats import binomtest as _binom_test\n",
    "    def _binom_pvals(array, n, p, alternative):\n",
    "        arr = np.asarray(array, dtype=int)\n",
    "        n = int(n)\n",
    "        return [_binom_test(int(k), n=n, p=p, alternative=alternative).pvalue for k in arr]\n",
    "except ImportError:\n",
    "    from scipy.stats import binom_test as _binom_test\n",
    "    def _binom_pvals(array, n, p, alternative):\n",
    "        arr = np.asarray(array, dtype=int)\n",
    "        n = int(n)\n",
    "        return [_binom_test(int(k), n=n, p=p, alternative=alternative) for k in arr]\n",
    "\n",
    "# Patch in das Paket injizieren\n",
    "BorutaShap.binomial_H0_test = staticmethod(_binom_pvals)\n",
    "\n",
    "# Modell definieren (du kannst deine Hyperparameter beibehalten/ändern)\n",
    "import lightgbm as lgb\n",
    "model = lgb.LGBMRegressor(\n",
    "    n_estimators=300, learning_rate=0.08, num_leaves=31,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# BorutaShap ausführen (Xtr_df: DataFrame, y_train: Ziel)\n",
    "fs = BorutaShap(model=model, importance_measure='shap', classification=False)\n",
    "fs.fit(X=Xtr_df, y=y_train, n_trials=25, sample=False, normalize=True, verbose=False)\n",
    "\n",
    "# Ausgewählte Features holen\n",
    "Xtr_sel = fs.Subset()                        # subset des TRAIN mit Keep-Features\n",
    "selected_cols = list(Xtr_sel.columns)\n",
    "\n",
    "print(f\"{len(selected_cols)} ausgewählte Features:\")\n",
    "print(selected_cols)\n",
    "\n",
    "# (Optional) Weitere Splits gleich filtern:\n",
    "# Xval_sel = Xval_df[selected_cols]\n",
    "# Xtest_sel = Xtest_df[selected_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58947cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MAE (RGF auf Boruta-SHAP-Features): 1186.6612\n"
     ]
    }
   ],
   "source": [
    "# --- 0) Sicherstellen, dass rgf (rgf-python) installiert ist ---\n",
    "import sys, subprocess, importlib\n",
    "def ensure_import(mod, pip_name=None):\n",
    "    try:\n",
    "        return importlib.import_module(mod)\n",
    "    except ModuleNotFoundError:\n",
    "        if pip_name is None:\n",
    "            pip_name = mod\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "        return importlib.import_module(mod)\n",
    "\n",
    "rgf_sklearn = ensure_import(\"rgf.sklearn\", \"rgf-python\")\n",
    "from rgf.sklearn import RGFRegressor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# --- Validation-Split anlegen, falls nicht vorhanden ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "if ('Xval_df' not in globals()) or ('y_val' not in globals()):\n",
    "    # Sicherstellen, dass y und X index-mäßig zueinander passen\n",
    "    y_train = pd.Series(y_train, index=Xtr_df.index)\n",
    "    Xtr_df, Xval_df, y_train, y_val = train_test_split(\n",
    "        Xtr_df, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Validation split created: train={Xtr_df.shape}, val={Xval_df.shape}\")\n",
    "\n",
    "\n",
    "# --- 1) Selektierte Features aus BorutaShap holen ---\n",
    "sel = getattr(fs, \"accepted\", None) or getattr(fs, \"accepted_features_\", None)\n",
    "if sel is None or len(sel) == 0:\n",
    "    raise ValueError(\"BorutaShap hat keine akzeptierten Features gefunden.\")\n",
    "\n",
    "sel_cols = list(sel)\n",
    "# Falls BorutaShap Indizes statt Namen liefert:\n",
    "if len(sel_cols) and not isinstance(sel_cols[0], str):\n",
    "    sel_cols = [Xtr_df.columns[int(i)] for i in sel_cols]\n",
    "\n",
    "# --- 2) Spalten über Train/Val(/Test) ausrichten ---\n",
    "def align_to_cols(df, cols):\n",
    "    df2 = df.copy()\n",
    "    for c in cols:\n",
    "        if c not in df2.columns:\n",
    "            df2[c] = 0\n",
    "    return df2.reindex(columns=cols, fill_value=0)\n",
    "\n",
    "Xtr_sel  = align_to_cols(Xtr_df,  sel_cols)\n",
    "Xval_sel = align_to_cols(Xval_df, sel_cols)\n",
    "try:\n",
    "    Xte_sel = align_to_cols(Xte_df, sel_cols)\n",
    "except NameError:\n",
    "    Xte_sel = None\n",
    "\n",
    "# --- 3) (Optional) Log-Transform des Targets ---\n",
    "use_log = True\n",
    "y_tr = np.log1p(y_train) if use_log else y_train\n",
    "\n",
    "# --- 4) RGF trainieren ---\n",
    "rgf = RGFRegressor(\n",
    "    loss=\"LS\",\n",
    "    algorithm=\"RGF_Sib\",\n",
    "    max_leaf=3000,\n",
    "    l2=1.0,\n",
    "    min_samples_leaf=20,\n",
    "    n_iter=100,\n",
    "    normalize=True,\n",
    "    )\n",
    "rgf.fit(Xtr_sel.values, y_tr)\n",
    "\n",
    "# --- 5) Validierung ---\n",
    "y_pred = rgf.predict(Xval_sel.values)\n",
    "if use_log:\n",
    "    y_pred = np.expm1(y_pred)\n",
    "\n",
    "val_mae = mean_absolute_error(y_val, y_pred)\n",
    "print(\"Val MAE (RGF auf Boruta-SHAP-Features):\", round(val_mae, 4))\n",
    "\n",
    "# --- 6) Optional: Test ---\n",
    "if Xte_sel is not None:\n",
    "    y_test_pred = rgf.predict(Xte_sel.values)\n",
    "    if use_log:\n",
    "        y_test_pred = np.expm1(y_test_pred)\n",
    "    # Beispiel: speichern\n",
    "    # pd.Series(y_test_pred, index=Xte_sel.index).to_csv(\"rgf_predictions.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e4d6f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen blend weight w=0.0571 (RF share) on validation\n",
      "Blend(RF, HistGB, log-target) -> R²=0.5527 | RMSE=1967.07 | MAE=1185.29\n",
      "[w=0.25] -> R²=0.5488 | RMSE=1975.48 | MAE=1174.45\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# ===== Eingaben vorausgesetzt: Xtr_df, y_train, Xval_df, y_val =====\n",
    "# Falls Series/DataFrames: sicher als np.array\n",
    "y_tr_log = np.log1p(y_train)\n",
    "\n",
    "# --- Modelle (nimm gern deine bisherigen Hyperparameter) ---\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300, max_features='sqrt', n_jobs=-1, random_state=42\n",
    ")\n",
    "hgb = HistGradientBoostingRegressor(\n",
    "    max_iter=300, learning_rate=0.08, max_leaf_nodes=31,\n",
    "    early_stopping=True, random_state=42\n",
    ")\n",
    "\n",
    "# --- Training auf log1p-Target ---\n",
    "rf.fit(Xtr_df, y_tr_log)\n",
    "hgb.fit(Xtr_df, y_tr_log)\n",
    "\n",
    "# --- Vorhersagen (Train für Smearing, Val für Metriken) ---\n",
    "rf_log_tr  = rf.predict(Xtr_df)\n",
    "hgb_log_tr = hgb.predict(Xtr_df)\n",
    "\n",
    "rf_log_val  = rf.predict(Xval_df)\n",
    "hgb_log_val = hgb.predict(Xval_df)\n",
    "\n",
    "# --- Retransformation: exmp1 oder (optional) Duan-Smearing ---\n",
    "use_smearing = True\n",
    "\n",
    "if use_smearing:\n",
    "    S_rf  = float(np.mean(np.exp(y_tr_log - rf_log_tr)))\n",
    "    S_hgb = float(np.mean(np.exp(y_tr_log - hgb_log_tr)))\n",
    "    rf_val  = S_rf  * np.exp(rf_log_val)  - 1.0\n",
    "    hgb_val = S_hgb * np.exp(hgb_log_val) - 1.0\n",
    "else:\n",
    "    rf_val  = np.expm1(rf_log_val)\n",
    "    hgb_val = np.expm1(hgb_log_val)\n",
    "\n",
    "rf_val  = np.asarray(rf_val)\n",
    "hgb_val = np.asarray(hgb_val)\n",
    "yv      = np.asarray(y_val)\n",
    "\n",
    "# --- Optimales Blend-Gewicht (für MSE/RMSE) analytisch ---\n",
    "def best_w_mse(y, a, b):\n",
    "    d = a - b\n",
    "    denom = float(np.dot(d, d))\n",
    "    if denom == 0.0:\n",
    "        return 0.5\n",
    "    w = float(np.dot(y - b, d) / denom)\n",
    "    return float(np.clip(w, 0.0, 1.0))\n",
    "\n",
    "w_star = best_w_mse(yv, rf_val, hgb_val)\n",
    "\n",
    "# --- Blends & Metriken ---\n",
    "def metrics(y, yhat):\n",
    "    rmse = mean_squared_error(y, yhat, squared=False)\n",
    "    mae  = mean_absolute_error(y, yhat)\n",
    "    r2   = r2_score(y, yhat)\n",
    "    return r2, rmse, mae\n",
    "\n",
    "# optimaler Blend\n",
    "blend_star = w_star * rf_val + (1.0 - w_star) * hgb_val\n",
    "r2_s, rmse_s, mae_s = metrics(yv, blend_star)\n",
    "\n",
    "# Vergleich: dein altes w=0.25 (RF-Anteil)\n",
    "w_old = 0.25\n",
    "blend_old = w_old * rf_val + (1.0 - w_old) * hgb_val\n",
    "r2_o, rmse_o, mae_o = metrics(yv, blend_old)\n",
    "\n",
    "print(f\"Chosen blend weight w={w_star:.4f} (RF share) on validation\")\n",
    "print(f\"Blend(RF, HistGB, log-target) -> R²={r2_s:.4f} | RMSE={rmse_s:.2f} | MAE={mae_s:.2f}\")\n",
    "print(f\"[w=0.25] -> R²={r2_o:.4f} | RMSE={rmse_o:.2f} | MAE={mae_o:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d53d5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best w (MAE) = 0.3960\n",
      "R²=0.5408 | RMSE=1992.91 | MAE=1172.10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Voraussetzung: yv (y_val), rf_val, hgb_val sind bereits auf ORIGINALSKALA\n",
    "# (also nach exmp1 bzw. Smearing rücktransformiert!)\n",
    "\n",
    "def best_w_mae_grid(y, a, b, coarse=0.01, fine=0.001, window=0.03):\n",
    "    # Grobe Suche\n",
    "    ws  = np.arange(0.0, 1.0 + coarse, coarse)\n",
    "    maes = [mean_absolute_error(y, w*a + (1-w)*b) for w in ws]\n",
    "    w0 = float(ws[int(np.argmin(maes))])\n",
    "\n",
    "    # Feinsuche um das Minimum herum\n",
    "    wL = max(0.0, w0 - window)\n",
    "    wU = min(1.0, w0 + window)\n",
    "    ws2  = np.arange(wL, wU + fine, fine)\n",
    "    maes2 = [mean_absolute_error(y, w*a + (1-w)*b) for w in ws2]\n",
    "    w_star = float(ws2[int(np.argmin(maes2))])\n",
    "    return w_star\n",
    "\n",
    "w_mae = best_w_mae_grid(yv, rf_val, hgb_val)\n",
    "blend_mae = w_mae * rf_val + (1 - w_mae) * hgb_val\n",
    "\n",
    "print(f\"Best w (MAE) = {w_mae:.4f}\")\n",
    "print(\"R²={:.4f} | RMSE={:.2f} | MAE={:.2f}\".format(\n",
    "    r2_score(yv, blend_mae),\n",
    "    mean_squared_error(yv, blend_mae, squared=False),\n",
    "    mean_absolute_error(yv, blend_mae),\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa635a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/18] MAE=1166.120 | RMSE=1997.9 | R²=0.5385  <- {'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0.0, 'reg_lambda': 2.0, 'learning_rate': 0.03}\n",
      "[02/18] MAE=1167.595 | RMSE=2001.9 | R²=0.5367  <- {'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0, 'colsample_bytree': 0.7, 'gamma': 0, 'reg_alpha': 0.0, 'reg_lambda': 0.5, 'learning_rate': 0.05}\n",
      "[03/18] MAE=1164.772 | RMSE=1988.5 | R²=0.5429  <- {'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 1.0, 'gamma': 0, 'reg_alpha': 0.01, 'reg_lambda': 2.0, 'learning_rate': 0.07}\n",
      "[04/18] MAE=1174.033 | RMSE=2025.3 | R²=0.5258  <- {'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 0.9, 'gamma': 0.5, 'reg_alpha': 0.0, 'reg_lambda': 0.5, 'learning_rate': 0.07}\n",
      "[05/18] MAE=1174.084 | RMSE=2024.4 | R²=0.5262  <- {'max_depth': 3, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 0.7, 'gamma': 0.5, 'reg_alpha': 0.01, 'reg_lambda': 2.0, 'learning_rate': 0.07}\n",
      "[06/18] MAE=1173.204 | RMSE=2023.1 | R²=0.5268  <- {'max_depth': 3, 'min_child_weight': 5, 'subsample': 1.0, 'colsample_bytree': 0.7, 'gamma': 0.5, 'reg_alpha': 0.0, 'reg_lambda': 2.0, 'learning_rate': 0.07}\n",
      "[07/18] MAE=1180.232 | RMSE=1995.7 | R²=0.5395  <- {'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.7, 'gamma': 0, 'reg_alpha': 0.1, 'reg_lambda': 0.5, 'learning_rate': 0.07}\n",
      "[08/18] MAE=1165.256 | RMSE=1977.6 | R²=0.5479  <- {'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.7, 'colsample_bytree': 1.0, 'gamma': 0.5, 'reg_alpha': 0.01, 'reg_lambda': 2.0, 'learning_rate': 0.07}\n",
      "[09/18] MAE=1158.615 | RMSE=1980.9 | R²=0.5464  <- {'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.9, 'colsample_bytree': 0.8, 'gamma': 0.5, 'reg_alpha': 0.1, 'reg_lambda': 2.0, 'learning_rate': 0.03}\n",
      "[10/18] MAE=1162.218 | RMSE=1980.7 | R²=0.5464  <- {'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.5, 'reg_alpha': 0.01, 'reg_lambda': 1.0, 'learning_rate': 0.05}\n",
      "[11/18] MAE=1158.020 | RMSE=1978.4 | R²=0.5475  <- {'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.7, 'gamma': 0.5, 'reg_alpha': 0.01, 'reg_lambda': 1.0, 'learning_rate': 0.03}\n",
      "[12/18] MAE=1159.507 | RMSE=1972.8 | R²=0.5500  <- {'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.9, 'colsample_bytree': 0.8, 'gamma': 0.5, 'reg_alpha': 0.01, 'reg_lambda': 2.0, 'learning_rate': 0.1}\n",
      "[13/18] MAE=1167.567 | RMSE=1978.5 | R²=0.5474  <- {'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.5, 'reg_alpha': 0.1, 'reg_lambda': 2.0, 'learning_rate': 0.1}\n",
      "[14/18] MAE=1159.901 | RMSE=1980.1 | R²=0.5467  <- {'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.5, 'reg_alpha': 0.0, 'reg_lambda': 0.5, 'learning_rate': 0.03}\n",
      "[15/18] MAE=1181.382 | RMSE=2003.5 | R²=0.5359  <- {'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 1.0, 'gamma': 0, 'reg_alpha': 0.01, 'reg_lambda': 1.0, 'learning_rate': 0.1}\n",
      "[16/18] MAE=1159.934 | RMSE=1973.4 | R²=0.5498  <- {'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0.5, 'reg_alpha': 0.1, 'reg_lambda': 1.0, 'learning_rate': 0.03}\n",
      "[17/18] MAE=1182.007 | RMSE=2003.0 | R²=0.5362  <- {'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.8, 'colsample_bytree': 1.0, 'gamma': 0, 'reg_alpha': 0.1, 'reg_lambda': 2.0, 'learning_rate': 0.07}\n",
      "[18/18] MAE=1167.236 | RMSE=1979.6 | R²=0.5470  <- {'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.7, 'colsample_bytree': 0.9, 'gamma': 0, 'reg_alpha': 0.0, 'reg_lambda': 1.0, 'learning_rate': 0.05}\n",
      "\n",
      "Bestes XGB (nach MAE) – ohne FS, alle Features\n",
      "{'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.7, 'gamma': 0.5, 'reg_alpha': 0.01, 'reg_lambda': 1.0, 'learning_rate': 0.03}\n",
      "R²=0.5475 | RMSE=1978.4 | MAE=1158.02 | XGBoost 3.0.4 | Log-Target=True\n"
     ]
    }
   ],
   "source": [
    "# XGBoost auf ALLEN Features (ohne FS), MAE-Optimierung, robust über XGB 1.x/2.x\n",
    "# Voraussetzungen: Xtr_df, y_train, Xval_df, y_val  (Pandas, numerisch encodiert)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, warnings, inspect\n",
    "from xgboost import XGBRegressor, __version__ as xgb_version\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "# ---------- 0) Spalten ausrichten (falls Val/Test Dummies fehlen) ----------\n",
    "def align_to_cols(df_ref: pd.DataFrame, df_to_fix: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = df_to_fix.copy()\n",
    "    missing = [c for c in df_ref.columns if c not in df2.columns]\n",
    "    for c in missing:\n",
    "        df2[c] = 0\n",
    "    # Extraspalten in Val, die Train nicht hat, verwerfen:\n",
    "    df2 = df2.reindex(columns=df_ref.columns, fill_value=0)\n",
    "    return df2\n",
    "\n",
    "Xval_all = align_to_cols(Xtr_df, Xval_df)\n",
    "\n",
    "# Optional: Nur numerische Spalten verwenden (Fehler robust umgehen)\n",
    "num_cols = Xtr_df.select_dtypes(include=[np.number, \"bool\"]).columns\n",
    "if len(num_cols) != Xtr_df.shape[1]:\n",
    "    warnings.warn(\n",
    "        f\"{Xtr_df.shape[1] - len(num_cols)} nicht-numerische Spalten in Xtr_df gefunden – sie werden ignoriert.\"\n",
    "    )\n",
    "Xtr_all  = Xtr_df[num_cols].astype(float)\n",
    "Xval_all = Xval_all[num_cols].astype(float)\n",
    "\n",
    "# ---------- 1) MAE-Ziel + optionales Log-Target ----------\n",
    "use_log = True  # bei stark rechtsschiefen Targets ggf. auf True setzen\n",
    "y_tr = np.log1p(y_train) if use_log else y_train\n",
    "y_va = np.log1p(y_val)   if use_log else y_val\n",
    "\n",
    "# ---------- 2) Fit-Signatur prüfen (für Early Stopping / eval_metric / verbose) ----------\n",
    "fit_sig_params = inspect.signature(XGBRegressor().fit).parameters\n",
    "supports_callbacks          = \"callbacks\" in fit_sig_params\n",
    "supports_es_rounds_in_fit   = \"early_stopping_rounds\" in fit_sig_params\n",
    "supports_eval_metric_in_fit = \"eval_metric\" in fit_sig_params\n",
    "supports_verbose_in_fit     = \"verbose\" in fit_sig_params\n",
    "\n",
    "# ---------- 3) Param-Space (kleine Random-Search für Speed) ----------\n",
    "param_space = {\n",
    "    \"max_depth\":        [3, 4, 5, 6],\n",
    "    \"min_child_weight\": [1, 2, 5],\n",
    "    \"subsample\":        [0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"gamma\":            [0, 0.5],\n",
    "    \"reg_alpha\":        [0.0, 0.01, 0.1],\n",
    "    \"reg_lambda\":       [0.5, 1.0, 2.0],\n",
    "    \"learning_rate\":    [0.03, 0.05, 0.07, 0.1],\n",
    "}\n",
    "\n",
    "def sample_params(space, n=18, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    keys = list(space.keys())\n",
    "    return [{k: rng.choice(space[k]) for k in keys} for _ in range(n)]\n",
    "\n",
    "trials = sample_params(param_space, n=18, seed=42)\n",
    "\n",
    "# Gemeinsame Defaults – bewusst 'reg:squarederror' für maximale Kompatibilität\n",
    "common = dict(\n",
    "    objective=\"reg:squarederror\",  # sicher in 1.x und 2.x\n",
    "    tree_method=\"hist\",            # schnell; falls GPU verfügbar -> \"gpu_hist\"\n",
    "    n_estimators=4000,             # hoch + Early Stopping\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# ---------- 4) Training + Early Stopping (robust je nach Version) ----------\n",
    "best = {\"mae\": np.inf, \"params\": None, \"model\": None, \"scores\": None}\n",
    "\n",
    "for i, p in enumerate(trials, 1):\n",
    "    # eval_metric möglichst in den Konstruktor setzen (funktioniert in 1.x/2.x)\n",
    "    model = XGBRegressor(eval_metric=\"mae\", **common, **p)\n",
    "\n",
    "    if supports_callbacks:\n",
    "        # Neuer Weg: via callbacks (XGB 2.x und manche 1.x)\n",
    "        try:\n",
    "            from xgboost.callback import EarlyStopping\n",
    "            model.fit(\n",
    "                Xtr_all, y_tr,\n",
    "                eval_set=[(Xval_all, y_va)],\n",
    "                callbacks=[EarlyStopping(rounds=150, save_best=True)]\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Fallback, falls callbacks doch nicht akzeptiert werden\n",
    "            kw = dict(eval_set=[(Xval_all, y_va)])\n",
    "            if supports_eval_metric_in_fit:\n",
    "                kw[\"eval_metric\"] = \"mae\"\n",
    "            if supports_es_rounds_in_fit:\n",
    "                kw[\"early_stopping_rounds\"] = 150\n",
    "            if supports_verbose_in_fit:\n",
    "                kw[\"verbose\"] = False\n",
    "            model.fit(Xtr_all, y_tr, **kw)\n",
    "    else:\n",
    "        # Alter Weg: eval_metric + early_stopping_rounds direkt in fit\n",
    "        kw = dict(eval_set=[(Xval_all, y_va)])\n",
    "        if supports_eval_metric_in_fit:\n",
    "            kw[\"eval_metric\"] = \"mae\"\n",
    "        if supports_es_rounds_in_fit:\n",
    "            kw[\"early_stopping_rounds\"] = 150\n",
    "        if supports_verbose_in_fit:\n",
    "            kw[\"verbose\"] = False\n",
    "        model.fit(Xtr_all, y_tr, **kw)\n",
    "\n",
    "    # Vorhersage (ggf. Rücktransformation)\n",
    "    z_hat = model.predict(Xval_all)\n",
    "    y_pred = np.expm1(z_hat) if use_log else z_hat\n",
    "\n",
    "    mae  = mean_absolute_error(y_val, y_pred)\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    r2   = r2_score(y_val, y_pred)\n",
    "\n",
    "    if mae < best[\"mae\"]:\n",
    "        best.update({\"mae\": mae, \"params\": p, \"model\": model, \"scores\": (r2, rmse, mae)})\n",
    "\n",
    "    print(f\"[{i:02d}/{len(trials)}] MAE={mae:.3f} | RMSE={rmse:.1f} | R²={r2:.4f}  <- {p}\")\n",
    "\n",
    "# ---------- 5) Resultate ----------\n",
    "print(\"\\nBestes XGB (nach MAE) – ohne FS, alle Features\")\n",
    "print(best[\"params\"])\n",
    "r2, rmse, mae = best[\"scores\"]\n",
    "print(f\"R²={r2:.4f} | RMSE={rmse:.1f} | MAE={mae:.2f} | XGBoost {xgb_version} | Log-Target={use_log}\")\n",
    "\n",
    "best_model = best[\"model\"]\n",
    "# Beispiel-Predictions (Originalskala):\n",
    "y_val_pred = best_model.predict(Xval_all)\n",
    "if use_log:\n",
    "    y_val_pred = np.expm1(y_val_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a1669a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert: xgb_final.pkl xgb_final_booster.json xgb_columns.txt xgb_meta.json\n"
     ]
    }
   ],
   "source": [
    "# FINAL: bestes XGB neu auf TRAIN+VAL fitten, speichern und später für neue Testdaten nutzen\n",
    "# Voraussetzungen aus deinem Lauf:\n",
    "# - best[\"model\"]  (das validierungs-beste Modell)\n",
    "# - best[\"params\"] (beste Param-Kombi aus der Suche)\n",
    "# - Xtr_all, Xval_all (alle Features, numerisch, spalten-ausgerichtet)\n",
    "# - y_train, y_val (Originalskala)\n",
    "# - use_log (True/False)\n",
    "# HINWEIS: Wenn du Xtr_all/Xval_all nicht mehr im RAM hast, bau sie identisch wieder auf.\n",
    "\n",
    "import numpy as np, pandas as pd, json, joblib\n",
    "from xgboost import XGBRegressor, __version__ as xgb_version\n",
    "\n",
    "# --- 1) Full data zusammenstellen (im selben Feature-Raum) ---\n",
    "X_full = pd.concat([Xtr_all, Xval_all], axis=0)\n",
    "y_full = pd.concat([y_train, y_val], axis=0)\n",
    "y_full_tr = np.log1p(y_full) if use_log else y_full\n",
    "\n",
    "# --- 2) Beste Parametrisierung aus dem validierten Modell holen ---\n",
    "best_model = best[\"model\"]\n",
    "params_used = best_model.get_xgb_params().copy()\n",
    "\n",
    "# Falls Early Stopping aktiv war: best_iteration verwenden\n",
    "best_iter = getattr(best_model, \"best_iteration\", None)\n",
    "if best_iter is not None and best_iter > 0:\n",
    "    params_used[\"n_estimators\"] = int(best_iter)\n",
    "\n",
    "# Sicherheits-Defaults:\n",
    "params_used.setdefault(\"objective\", \"reg:squarederror\")\n",
    "params_used.setdefault(\"tree_method\", \"hist\")\n",
    "params_used.setdefault(\"n_jobs\", -1)\n",
    "params_used.setdefault(\"random_state\", 42)\n",
    "\n",
    "# --- 3) Finales Modell auf TRAIN+VAL fitten (ohne eval_set) ---\n",
    "final_model = XGBRegressor(**params_used)\n",
    "final_model.fit(X_full, y_full_tr)\n",
    "\n",
    "# --- 4) (Optional) Duan-Smearing-Faktor für Log-Target schätzen ---\n",
    "smear = 1.0\n",
    "if use_log:\n",
    "    z_hat_full = final_model.predict(X_full)\n",
    "    smear = float(np.mean(np.exp(y_full_tr - z_hat_full)))  # E[exp(resid)]\n",
    "\n",
    "# --- 5) Artefakte speichern ---\n",
    "final_model_path   = \"xgb_final.pkl\"                    # sklearn-Wrapper\n",
    "booster_json_path  = \"xgb_final_booster.json\"           # Booster (versionstabil)\n",
    "columns_path       = \"xgb_columns.txt\"                  # Spaltenreihenfolge\n",
    "meta_path          = \"xgb_meta.json\"                    # Meta-Infos\n",
    "\n",
    "joblib.dump(final_model, final_model_path)\n",
    "final_model.get_booster().save_model(booster_json_path)\n",
    "pd.Series(X_full.columns).to_csv(columns_path, index=False, header=False)\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"use_log\": bool(use_log),\n",
    "        \"smear\": float(smear),\n",
    "        \"xgboost_version\": xgb_version,\n",
    "        \"params_used\": params_used\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Gespeichert:\", final_model_path, booster_json_path, columns_path, meta_path)\n",
    "\n",
    "# --- 6) Hilfsfunktionen für zukünftige Nutzung auf NEUEN Testdaten ---\n",
    "def load_artifacts(prefix=\"xgb_\"):\n",
    "    model   = joblib.load(\"xgb_final.pkl\")\n",
    "    with open(\"xgb_meta.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    cols = pd.read_csv(\"xgb_columns.txt\", header=None).iloc[:,0].tolist()\n",
    "    return model, cols, meta\n",
    "\n",
    "def align_to_cols(df, cols):\n",
    "    df2 = df.copy()\n",
    "    missing = [c for c in cols if c not in df2.columns]\n",
    "    for c in missing:\n",
    "        df2[c] = 0\n",
    "    return df2.reindex(columns=cols, fill_value=0)\n",
    "\n",
    "def predict_on_new(df_new: pd.DataFrame):\n",
    "    model, cols, meta = load_artifacts()\n",
    "    X_new = align_to_cols(df_new, cols).astype(float)\n",
    "    z = model.predict(X_new)\n",
    "    if meta[\"use_log\"]:\n",
    "        y_hat = np.expm1(z) * float(meta.get(\"smear\", 1.0))\n",
    "    else:\n",
    "        y_hat = z\n",
    "    return pd.Series(y_hat, index=X_new.index, name=\"prediction\")\n",
    "\n",
    "# Beispiel:\n",
    "# new_preds = predict_on_new(Xte_df)\n",
    "# new_preds.to_csv(\"xgb_preds_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "37b62b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5] MAE=1198.108 | RMSE=2029.10 | R²=0.5234 | smear=1.14183\n",
      "[Fold 2/5] MAE=1216.140 | RMSE=1977.97 | R²=0.5459 | smear=1.14021\n",
      "[Fold 3/5] MAE=1199.452 | RMSE=1924.67 | R²=0.5468 | smear=1.14095\n",
      "[Fold 4/5] MAE=1196.619 | RMSE=1980.48 | R²=0.5455 | smear=1.14037\n",
      "[Fold 5/5] MAE=1185.410 | RMSE=1920.05 | R²=0.5638 | smear=1.14094\n",
      "\n",
      "=== OOF (K-Fold) ===\n",
      "MAE=1199.146 | RMSE=1966.87 | R²=0.5450\n",
      "Gespeichert nach: C:\\Users\\phill\\Desktop\\Python_Basics\\ML_Allstate_claims_group2\\cv_artifacts_xgb\n",
      "RGF (auf letztem Fold) MAE=1173.329\n"
     ]
    }
   ],
   "source": [
    "# ===================== XGB K-Fold CV (OOF) + optional RGF =====================\n",
    "# Voraussetzungen: vorverarbeitete Trainingsdaten als Pandas:\n",
    "#   - Xtr_df, y_train  (und optional Xval_df, y_val -> werden zu X_all/y_all zusammengeführt)\n",
    "# Nutzt dein bestes Param-Set:\n",
    "#   {'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.7,\n",
    "#    'gamma': 0.5, 'reg_alpha': 0.01, 'reg_lambda': 1.0, 'learning_rate': 0.03}\n",
    "#\n",
    "# Hinweise:\n",
    "# - Kein Early Stopping (kompatibel mit deiner XGBoost-Version). Wähle n_estimators konservativ.\n",
    "# - Log-Target + Duan-Smearing pro Fold korrekt angewandt.\n",
    "# - Speichert Fold-Modelle + Meta (Smearing je Fold) für spätere Nutzung/Blends.\n",
    "\n",
    "import os, json, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, __version__ as xgb_version\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "N_FOLDS      = 5\n",
    "N_ESTIMATORS = 700        # konservativ ohne Early Stopping (bei Bedarf erhöhen/reduzieren)\n",
    "RANDOM_STATE = 42\n",
    "USE_LOG      = True        # wie bei dir\n",
    "USE_SMEARING = True\n",
    "\n",
    "SAVE_DIR = Path(\"./cv_artifacts_xgb\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BEST_PARAMS = {\n",
    "    \"max_depth\": 5,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"gamma\": 0.5,\n",
    "    \"reg_alpha\": 0.01,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"learning_rate\": 0.03,\n",
    "}\n",
    "\n",
    "# Optionale weitere Regularisierung (einfach mal testen):\n",
    "BEST_PARAMS.update({\n",
    "     \"colsample_bylevel\": 0.9,\n",
    "     \"colsample_bynode\": 0.9,\n",
    "     \"grow_policy\": \"lossguide\",  # nur mit tree_method=\"hist\" stabil nutzen\n",
    "     \"max_leaves\": 64,\n",
    "     \"max_bin\": 64,\n",
    "})\n",
    "\n",
    "def parse_ver(v):\n",
    "    try:\n",
    "        major, minor, *_ = v.split(\"+\")[0].split(\".\")\n",
    "        return (int(major), int(minor))\n",
    "    except Exception:\n",
    "        return (1, 7)\n",
    "\n",
    "XGB_MM = parse_ver(xgb_version)\n",
    "OBJECTIVE = \"reg:absoluteerror\" if XGB_MM >= (2, 0) else \"reg:squarederror\"\n",
    "\n",
    "# ---------------- Daten zusammenführen ----------------\n",
    "def build_Xy_for_cv():\n",
    "    # Nutzt train + val, falls vorhanden; sonst nur train\n",
    "    if \"Xval_df\" in globals() and \"y_val\" in globals() and len(Xval_df) == len(y_val):\n",
    "        X_all = pd.concat([Xtr_df, Xval_df], axis=0)\n",
    "        y_all = pd.concat([y_train, y_val], axis=0)\n",
    "    else:\n",
    "        X_all, y_all = Xtr_df.copy(), y_train.copy()\n",
    "    return X_all, y_all\n",
    "\n",
    "X_all, y_all = build_Xy_for_cv()\n",
    "assert isinstance(X_all, pd.DataFrame) and len(X_all) == len(y_all)\n",
    "\n",
    "# ---------------- Helper: log/exp mit Smearing ----------------\n",
    "def to_log(y):\n",
    "    return np.log1p(y) if USE_LOG else y\n",
    "\n",
    "def from_log(z, smear=1.0):\n",
    "    if USE_LOG:\n",
    "        out = np.expm1(z)\n",
    "        if USE_SMEARING:\n",
    "            out = out * smear\n",
    "        return out\n",
    "    return z\n",
    "\n",
    "# ---------------- K-Fold Training ----------------\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "oof_pred = np.zeros(len(X_all), dtype=float)\n",
    "fold_smears = []\n",
    "fold_models = []\n",
    "fold_scores = []\n",
    "\n",
    "common = dict(\n",
    "    objective=OBJECTIVE,\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    tree_method=\"hist\",   # stabil/schnell auf CPU; wenn GPU vorhanden: \"gpu_hist\"\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "common.update(BEST_PARAMS)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(X_all), 1):\n",
    "    X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "    y_tr, y_va = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "\n",
    "    y_tr_z = to_log(y_tr)  # im Lograum trainieren\n",
    "    y_va_z = to_log(y_va)  # nur für kompatible eval_set, falls du mal evals nutzt\n",
    "\n",
    "    model = XGBRegressor(**common)\n",
    "    # Kein Early Stopping (kompatibel zu deiner Installation)\n",
    "    model.fit(X_tr, y_tr_z, verbose=False)\n",
    "\n",
    "    # Smearing aus Train-Residuen im Lograum\n",
    "    smear = 1.0\n",
    "    if USE_LOG and USE_SMEARING:\n",
    "        z_hat_tr = model.predict(X_tr)\n",
    "        resid_tr = y_tr_z - z_hat_tr\n",
    "        smear = float(np.mean(np.exp(resid_tr)))\n",
    "\n",
    "    # OOF-Predictions auf Originalskala\n",
    "    z_hat_va = model.predict(X_va)\n",
    "    y_hat_va = from_log(z_hat_va, smear=smear)\n",
    "    oof_pred[va_idx] = y_hat_va\n",
    "\n",
    "    # Metriken je Fold\n",
    "    mae  = mean_absolute_error(y_va, y_hat_va)\n",
    "    rmse = mean_squared_error(y_va, y_hat_va, squared=False)\n",
    "    r2   = r2_score(y_va, y_hat_va)\n",
    "    fold_scores.append((mae, rmse, r2))\n",
    "    fold_smears.append(smear)\n",
    "    fold_models.append(model)\n",
    "\n",
    "    print(f\"[Fold {fold}/{N_FOLDS}] MAE={mae:.3f} | RMSE={rmse:.2f} | R²={r2:.4f} | smear={smear:.5f}\")\n",
    "\n",
    "# ---------------- Gesamt-OOF ----------------\n",
    "oof_mae  = mean_absolute_error(y_all, oof_pred)\n",
    "oof_rmse = mean_squared_error(y_all, oof_pred, squared=False)\n",
    "oof_r2   = r2_score(y_all, oof_pred)\n",
    "print(\"\\n=== OOF (K-Fold) ===\")\n",
    "print(f\"MAE={oof_mae:.3f} | RMSE={oof_rmse:.2f} | R²={oof_r2:.4f}\")\n",
    "\n",
    "# ---------------- Modelle & Meta speichern ----------------\n",
    "for i, m in enumerate(fold_models, 1):\n",
    "    with open(SAVE_DIR / f\"xgb_fold{i}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(m, f)\n",
    "\n",
    "meta = {\n",
    "    \"xgboost_version\": xgb_version,\n",
    "    \"n_folds\": N_FOLDS,\n",
    "    \"params\": common,\n",
    "    \"use_log\": USE_LOG,\n",
    "    \"use_smearing\": USE_SMEARING,\n",
    "    \"fold_smears\": fold_smears,\n",
    "}\n",
    "with open(SAVE_DIR / \"xgb_cv_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(f\"Gespeichert nach: {SAVE_DIR.resolve()}\")\n",
    "\n",
    "# ---------------- (Optional) Ensemble-Predict mit den Fold-Modellen ----------------\n",
    "def predict_cv_ensemble(X_new):\n",
    "    \"\"\"Mittel der Fold-Modelle mit je eigenem Smearing.\"\"\"\n",
    "    preds = []\n",
    "    for m, smear in zip(fold_models, fold_smears):\n",
    "        z = m.predict(X_new)\n",
    "        preds.append(from_log(z, smear=smear))\n",
    "    return np.mean(preds, axis=0)\n",
    "\n",
    "# Beispiel:\n",
    "# y_val_ens = predict_cv_ensemble(Xval_df)  # falls Xval_df existiert\n",
    "# print(\"Val-Ensemble MAE:\", mean_absolute_error(y_val, y_val_ens))\n",
    "\n",
    "# ---------------- (Optional) RGF zum Ensembling testen ----------------\n",
    "try:\n",
    "    from rgf.sklearn import RGFRegressor  # pip install rgf-python\n",
    "    rgf = RGFRegressor(\n",
    "        loss=\"LS\",\n",
    "        algorithm=\"RGF_Sib\",\n",
    "        max_leaf=2000,\n",
    "        l2=1.0,\n",
    "        min_samples_leaf=20,\n",
    "        n_iter=100,\n",
    "        normalize=True,\n",
    "    )\n",
    "    # Einfacher Holdout: nimm z.B. die letzte Fold-Validierung erneut\n",
    "    rgf.fit(X_tr.values, to_log(y_tr))      # im Lograum\n",
    "    z_rgf = rgf.predict(X_va.values)\n",
    "    y_rgf = from_log(z_rgf, smear=1.0)      # RGF-Smearing könntest du analog schätzen\n",
    "    print(f\"RGF (auf letztem Fold) MAE={mean_absolute_error(y_va, y_rgf):.3f}\")\n",
    "except Exception as e:\n",
    "    print(\"RGF nicht verfügbar (pip install rgf-python) oder Fehler beim Fit:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9fab84df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starte 5-Fold OOF für Basismodelle …\n",
      "  Fold 1: MAE xgb=1201.45 | hgb=1209.20 | rf=1225.12\n",
      "  Fold 2: MAE xgb=1199.08 | hgb=1205.97 | rf=1220.11\n",
      "  Fold 3: MAE xgb=1182.29 | hgb=1193.78 | rf=1203.64\n",
      "  Fold 4: MAE xgb=1174.96 | hgb=1185.27 | rf=1203.88\n",
      "  Fold 5: MAE xgb=1195.66 | hgb=1205.73 | rf=1221.09\n",
      "OOF-MAE xgb: 1190.69\n",
      "OOF-MAE hgb: 1199.99\n",
      "OOF-MAE rf: 1214.77\n",
      "\n",
      "=== Best Blend (OOF-MAE) ===\n",
      "Weights (xgb,hgb,rf) = [0.55 0.   0.45]\n",
      "Validation: R²=0.5397 | RMSE=1995.3 | MAE=1168.10\n",
      "\n",
      "=== Stacking (Meta-XGB auf OOF) ===\n",
      "Validation: R²=0.5316 | RMSE=2012.8 | MAE=1158.84\n",
      "\n",
      "[ERGEBNIS] Stacking schlägt Blending auf MAE – nimm Stacking als Final!\n"
     ]
    }
   ],
   "source": [
    "# ========= Blending & Stacking auf Basis deines besten XGB =========\n",
    "# - Basismodelle: XGB (deine Best-Params), HistGB, RandomForest\n",
    "# - Zielmetrik: MAE (mit Log-Target + Duan-Smearing für Rücktransformation)\n",
    "# - KFold OOF für sauberes Stacking, plus gewichtetes Blending\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor, __version__ as xgb_version\n",
    "\n",
    "# --------- 0) Daten ----------\n",
    "X_all = Xtr_df.copy()\n",
    "y_all = np.asarray(y_train).reshape(-1)\n",
    "X_val = Xval_df.copy()\n",
    "y_val_true = np.asarray(y_val).reshape(-1)\n",
    "\n",
    "# --------- 1) Log-Target + Smearing ----------\n",
    "USE_LOG = True\n",
    "USE_SMEAR = True\n",
    "\n",
    "def to_log(y):\n",
    "    return np.log1p(y) if USE_LOG else y\n",
    "\n",
    "def backtransform(z, smear=1.0):\n",
    "    if USE_LOG:\n",
    "        out = np.expm1(z)\n",
    "        return out * smear if USE_SMEAR else out\n",
    "    return z\n",
    "\n",
    "# --------- 2) XGBoost-Objective je Version ----------\n",
    "def major_minor(v: str):\n",
    "    try:\n",
    "        p = v.split(\"+\")[0].split(\".\")\n",
    "        return (int(p[0]), int(p[1]))\n",
    "    except Exception:\n",
    "        return (1, 7)\n",
    "\n",
    "XGB_MM = major_minor(xgb_version)\n",
    "XGB_OBJ = \"reg:absoluteerror\" if XGB_MM >= (2, 0) else \"reg:squarederror\"\n",
    "\n",
    "# --------- 3) Basismodelle definieren ----------\n",
    "# Deine Best-Params für XGB:\n",
    "XGB_BASE = dict(\n",
    "    max_depth=5, min_child_weight=1, subsample=0.8, colsample_bytree=0.7,\n",
    "    gamma=0.5, reg_alpha=0.01, reg_lambda=1.0, learning_rate=0.03,\n",
    "    n_estimators=1200,  # konservativ ohne Early-Stopping\n",
    "    tree_method=\"hist\", # GPU: \"gpu_hist\" wenn verfügbar\n",
    "    n_jobs=-1, random_state=42, objective=XGB_OBJ\n",
    ")\n",
    "\n",
    "def make_models():\n",
    "    xgb = XGBRegressor(**XGB_BASE)\n",
    "    hgb = HistGradientBoostingRegressor(\n",
    "        learning_rate=0.08, max_depth=6, max_leaf_nodes=63,\n",
    "        min_samples_leaf=20, l2_regularization=0.0, random_state=42\n",
    "    )\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=400, max_features=\"sqrt\", min_samples_leaf=1,\n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "    return {\"xgb\": xgb, \"hgb\": hgb, \"rf\": rf}\n",
    "\n",
    "# --------- 4) OOF-Stacking vorbereiten ----------\n",
    "KF = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_log_all = to_log(y_all)\n",
    "\n",
    "oof_preds = {name: np.zeros(len(X_all), dtype=float) for name in [\"xgb\",\"hgb\",\"rf\"]}\n",
    "val_preds = {name: np.zeros(len(X_val), dtype=float) for name in [\"xgb\",\"hgb\",\"rf\"]}\n",
    "\n",
    "print(\"[INFO] Starte 5-Fold OOF für Basismodelle …\")\n",
    "for fold, (tr_idx, va_idx) in enumerate(KF.split(X_all), 1):\n",
    "    X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "    y_tr_log, y_va_true = y_log_all[tr_idx], y_all[va_idx]\n",
    "\n",
    "    models = make_models()\n",
    "    for name, model in models.items():\n",
    "        # Train im Lograum\n",
    "        model.fit(X_tr, y_tr_log)\n",
    "\n",
    "        # Smearing auf Train\n",
    "        smear = 1.0\n",
    "        if USE_LOG and USE_SMEAR:\n",
    "            z_tr = model.predict(X_tr)\n",
    "            smear = float(np.mean(np.exp(y_tr_log - z_tr)))\n",
    "\n",
    "        # OOF auf Fold-Validierung (zur Meta-Train)\n",
    "        z_va = model.predict(X_va)\n",
    "        oof_preds[name][va_idx] = backtransform(z_va, smear)\n",
    "\n",
    "        # Gleichzeitig: Vorhersage auf globalem Validation-Set (für Meta-Test)\n",
    "        z_val = model.predict(X_val)\n",
    "        val_preds[name] += backtransform(z_val, smear) / KF.get_n_splits()\n",
    "\n",
    "    # Reporting pro Fold\n",
    "    mae_fold = {n: mean_absolute_error(y_va_true, oof_preds[n][va_idx]) for n in oof_preds}\n",
    "    print(f\"  Fold {fold}: MAE xgb={mae_fold['xgb']:.2f} | hgb={mae_fold['hgb']:.2f} | rf={mae_fold['rf']:.2f}\")\n",
    "\n",
    "# OOF-Metriken je Basismodell (Training)\n",
    "for n in oof_preds:\n",
    "    m = mean_absolute_error(y_all, oof_preds[n])\n",
    "    print(f\"OOF-MAE {n}: {m:.2f}\")\n",
    "\n",
    "# --------- 5) Blending (gewichtetes Mittel) – MAE auf OOF minimieren ----------\n",
    "# Gitter mit Schrittweite 0.05 – schnell & robust\n",
    "names = [\"xgb\", \"hgb\", \"rf\"]\n",
    "P = np.vstack([oof_preds[n] for n in names]).T  # (n_samples, n_models)\n",
    "P_val = np.vstack([val_preds[n] for n in names]).T\n",
    "\n",
    "best_blend = {\"mae\": np.inf, \"w\": None, \"val_metrics\": None}\n",
    "\n",
    "grid = np.arange(0.0, 1.05, 0.05)\n",
    "for w_x in grid:\n",
    "    for w_h in grid:\n",
    "        w_r = 1.0 - w_x - w_h\n",
    "        if w_r < 0 or w_r > 1: \n",
    "            continue\n",
    "        w = np.array([w_x, w_h, w_r])\n",
    "        y_oof_blend = P @ w\n",
    "        mae = mean_absolute_error(y_all, y_oof_blend)\n",
    "        if mae < best_blend[\"mae\"]:\n",
    "            y_val_blend = P_val @ w\n",
    "            r2  = r2_score(y_val_true, y_val_blend)\n",
    "            rmse= mean_squared_error(y_val_true, y_val_blend, squared=False)\n",
    "            mae_val = mean_absolute_error(y_val_true, y_val_blend)\n",
    "            best_blend.update(mae=mae, w=w, val_metrics=(r2, rmse, mae_val))\n",
    "\n",
    "print(\"\\n=== Best Blend (OOF-MAE) ===\")\n",
    "print(f\"Weights (xgb,hgb,rf) = {best_blend['w']}\")\n",
    "r2, rmse, mae = best_blend[\"val_metrics\"]\n",
    "print(f\"Validation: R²={r2:.4f} | RMSE={rmse:.1f} | MAE={mae:.2f}\")\n",
    "\n",
    "# --------- 6) Stacking (Meta-Learner) – MAE-Objective ----------\n",
    "# Meta-Features = OOF-Preds der Basismodelle (Originalskala)\n",
    "X_meta_train = pd.DataFrame({n: oof_preds[n] for n in names})\n",
    "X_meta_val   = pd.DataFrame({n: val_preds[n] for n in names})\n",
    "\n",
    "meta = XGBRegressor(\n",
    "    objective=(\"reg:absoluteerror\" if XGB_MM >= (2,0) else \"reg:squarederror\"),\n",
    "    n_estimators=500, max_depth=3, learning_rate=0.05,\n",
    "    subsample=1.0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0,\n",
    "    tree_method=\"hist\", n_jobs=-1, random_state=42\n",
    ")\n",
    "meta.fit(X_meta_train, y_all)\n",
    "\n",
    "y_val_stack = meta.predict(X_meta_val)\n",
    "mae_stack   = mean_absolute_error(y_val_true, y_val_stack)\n",
    "rmse_stack  = mean_squared_error(y_val_true, y_val_stack, squared=False)\n",
    "r2_stack    = r2_score(y_val_true, y_val_stack)\n",
    "\n",
    "print(\"\\n=== Stacking (Meta-XGB auf OOF) ===\")\n",
    "print(f\"Validation: R²={r2_stack:.4f} | RMSE={rmse_stack:.1f} | MAE={mae_stack:.2f}\")\n",
    "\n",
    "# --------- 7) Fazit ausgeben ----------\n",
    "if mae_stack < best_blend[\"val_metrics\"][2]:\n",
    "    print(\"\\n[ERGEBNIS] Stacking schlägt Blending auf MAE – nimm Stacking als Final!\")\n",
    "else:\n",
    "    print(\"\\n[ERGEBNIS] Blending schlägt (oder ≈) Stacking – nimm Blend mit den obigen Gewichten!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "64540b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] XGBoost 3.0.4 – starte 2 Trials mit 3-Fold CV (ohne Log-Target)…\n",
      "[01/2] XGB CV-MAE=1165.024  (depthwise, depth=4, leaves=-, lr=0.02, l1=0.1, l2=5.0, mcw=4, gamma=0.0, subs=0.7, cbt=0.7, cbl=0.8, cbn=0.7, mbin=128)\n",
      "[02/2] XGB CV-MAE=1156.991  (depthwise, depth=5, leaves=-, lr=0.03, l1=0.1, l2=2.0, mcw=1, gamma=1.0, subs=1.0, cbt=0.7, cbl=1.0, cbn=0.8, mbin=128)\n",
      "\n",
      "=== Bestes XGB (ohne Log-Target) ===\n",
      "{'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 0.7, 'colsample_bylevel': 1.0, 'colsample_bynode': 0.8, 'gamma': 1.0, 'reg_alpha': 0.1, 'reg_lambda': 2.0, 'max_bin': 128, 'use_lossguide': False, 'max_leaves': 127, 'grow_policy': 'depthwise'}\n",
      "Val: R²=0.5289 | RMSE=2018.6 | MAE=1162.17\n",
      "\n",
      "[INFO] Tune HistGB (kleiner Grid, MAE)…\n",
      "[1/5] HistGB MAE=1204.63  <- {'max_depth': None, 'max_leaf_nodes': 31, 'min_samples_leaf': 20, 'l2_regularization': 0.0}\n",
      "[2/5] HistGB MAE=1196.80  <- {'max_depth': None, 'max_leaf_nodes': 63, 'min_samples_leaf': 20, 'l2_regularization': 0.0}\n",
      "[3/5] HistGB MAE=1198.37  <- {'max_depth': None, 'max_leaf_nodes': 127, 'min_samples_leaf': 10, 'l2_regularization': 0.0}\n",
      "[4/5] HistGB MAE=1203.50  <- {'max_depth': 6, 'max_leaf_nodes': None, 'min_samples_leaf': 20, 'l2_regularization': 0.0}\n",
      "[5/5] HistGB MAE=1202.10  <- {'max_depth': 7, 'max_leaf_nodes': None, 'min_samples_leaf': 15, 'l2_regularization': 0.1}\n",
      "\n",
      "=== Bestes HistGB ===\n",
      "{'max_depth': None, 'max_leaf_nodes': 63, 'min_samples_leaf': 20, 'l2_regularization': 0.0}\n",
      "Val: MAE=1196.80\n",
      "\n",
      "=== Blend(XGB, HistGB) ===\n",
      "Bestes w (XGB-Anteil) = 0.795\n",
      "Val: R²=0.5444 | RMSE=1985.2 | MAE=1159.53\n"
     ]
    }
   ],
   "source": [
    "# ==== XGBoost (ohne Log-Target) + HistGB + Gewichts-Blending (MAE) ====\n",
    "# Voraussetzungen: Xtr_df, y_train, Xval_df, y_val (numerisch, gleiche Prepro)\n",
    "import numpy as np, pandas as pd, random, warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor, __version__ as xgb_version\n",
    "\n",
    "# ---------- 0) Utilities ----------\n",
    "def align_to_cols(df_ref: pd.DataFrame, df_to_fix: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = df_to_fix.copy()\n",
    "    missing = [c for c in df_ref.columns if c not in df2.columns]\n",
    "    for c in missing: df2[c] = 0\n",
    "    return df2.reindex(columns=df_ref.columns, fill_value=0)\n",
    "\n",
    "def major_minor(ver: str):\n",
    "    try:\n",
    "        p = ver.split(\"+\")[0].split(\".\")\n",
    "        return (int(p[0]), int(p[1]))\n",
    "    except Exception:\n",
    "        return (2,0)\n",
    "\n",
    "# ---------- 1) Daten vorbereiten (nur numerische Spalten) ----------\n",
    "num_cols = Xtr_df.select_dtypes(include=[np.number, \"bool\"]).columns\n",
    "if len(num_cols) != Xtr_df.shape[1]:\n",
    "    warnings.warn(f\"{Xtr_df.shape[1]-len(num_cols)} nicht-numerische Spalten – werden ignoriert.\")\n",
    "\n",
    "Xtr_all  = Xtr_df[num_cols].astype(float)\n",
    "Xval_all = align_to_cols(Xtr_all, Xval_df)[num_cols].astype(float)\n",
    "\n",
    "y_tr = np.asarray(y_train).reshape(-1)\n",
    "y_va = np.asarray(y_val).reshape(-1)\n",
    "\n",
    "# ---------- 2) XGB: Basis + lokaler Suchraum (ohne Log-Target) ----------\n",
    "BASE = dict(\n",
    "    max_depth=5, min_child_weight=1,\n",
    "    subsample=0.8, colsample_bytree=0.7,\n",
    "    gamma=0.5, reg_alpha=0.01, reg_lambda=1.0,\n",
    "    learning_rate=0.03\n",
    ")\n",
    "\n",
    "space = {\n",
    "    \"learning_rate\":    [0.02, 0.03, 0.04, 0.05],\n",
    "    \"max_depth\":        [4, 5, 6, 7],\n",
    "    \"min_child_weight\": [1, 2, 4, 6],\n",
    "    \"subsample\":        [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bylevel\":[0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bynode\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"gamma\":            [0.0, 0.2, 0.5, 1.0],\n",
    "    \"reg_alpha\":        [0.0, 0.005, 0.01, 0.05, 0.1],\n",
    "    \"reg_lambda\":       [0.5, 1.0, 2.0, 5.0],\n",
    "    \"max_bin\":          [128, 256, 512],\n",
    "    \"use_lossguide\":    [False, True],\n",
    "    \"max_leaves\":       [31, 63, 127]  # nur genutzt bei lossguide\n",
    "}\n",
    "\n",
    "def sample_params(n=40, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    trials = []\n",
    "    for _ in range(n):\n",
    "        p = {k: rng.choice(v) for k, v in space.items()}\n",
    "        p[\"grow_policy\"] = \"lossguide\" if p[\"use_lossguide\"] else \"depthwise\"\n",
    "        trials.append(p)\n",
    "    return trials\n",
    "\n",
    "N_TRIALS = 2\n",
    "CV_FOLDS = 3\n",
    "trials = sample_params(N_TRIALS, seed=42)\n",
    "\n",
    "XGB_MM = major_minor(xgb_version)\n",
    "OBJ = \"reg:absoluteerror\" if XGB_MM >= (2,0) else \"reg:squarederror\"\n",
    "\n",
    "COMMON_XGB = dict(\n",
    "    objective=OBJ,\n",
    "    tree_method=\"hist\",   # GPU: \"gpu_hist\" falls vorhanden\n",
    "    n_estimators=1500,    # moderat, kein Early Stopping (max. Kompatibilität)\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "def make_xgb(p: dict) -> XGBRegressor:\n",
    "    kws = COMMON_XGB.copy()\n",
    "    kws.update(BASE)\n",
    "    for k in [\"learning_rate\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\",\n",
    "              \"gamma\",\"reg_alpha\",\"reg_lambda\",\"max_bin\",\"colsample_bylevel\",\"colsample_bynode\"]:\n",
    "        kws[k] = p[k]\n",
    "    kws[\"grow_policy\"] = p[\"grow_policy\"]\n",
    "    if p[\"use_lossguide\"]:\n",
    "        kws[\"max_leaves\"] = p[\"max_leaves\"]\n",
    "    else:\n",
    "        kws.pop(\"max_leaves\", None)\n",
    "    return XGBRegressor(**kws)\n",
    "\n",
    "# ---------- 3) CV für XGB (MAE) ----------\n",
    "kf = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=42)\n",
    "best = {\"cv_mae\": np.inf, \"params\": None, \"model\": None, \"scores\": None}\n",
    "\n",
    "print(f\"[INFO] XGBoost {xgb_version} – starte {N_TRIALS} Trials mit {CV_FOLDS}-Fold CV (ohne Log-Target)…\")\n",
    "for t, p in enumerate(trials, 1):\n",
    "    fold_mae = []\n",
    "    for tr_idx, va_idx in kf.split(Xtr_all):\n",
    "        X_tr, X_va = Xtr_all.iloc[tr_idx], Xtr_all.iloc[va_idx]\n",
    "        y_tr_f, y_va_f = y_tr[tr_idx], y_tr[va_idx]\n",
    "        m = make_xgb(p)\n",
    "        m.fit(X_tr, y_tr_f)   # kein ES/Callbacks → maximale Kompatibilität\n",
    "        y_hat = m.predict(X_va)\n",
    "        fold_mae.append(mean_absolute_error(y_va_f, y_hat))\n",
    "    mean_mae = float(np.mean(fold_mae))\n",
    "    print(f\"[{t:02d}/{N_TRIALS}] XGB CV-MAE={mean_mae:.3f}  \"\n",
    "          f\"({p['grow_policy']}, depth={p['max_depth']}, leaves={p['max_leaves'] if p['use_lossguide'] else '-'}, \"\n",
    "          f\"lr={p['learning_rate']}, l1={p['reg_alpha']}, l2={p['reg_lambda']}, \"\n",
    "          f\"mcw={p['min_child_weight']}, gamma={p['gamma']}, subs={p['subsample']}, \"\n",
    "          f\"cbt={p['colsample_bytree']}, cbl={p['colsample_bylevel']}, cbn={p['colsample_bynode']}, mbin={p['max_bin']})\")\n",
    "    if mean_mae < best[\"cv_mae\"]:\n",
    "        best[\"cv_mae\"], best[\"params\"] = mean_mae, p\n",
    "\n",
    "# Refit bestes XGB auf vollem Train + Val-Metriken\n",
    "best_xgb = make_xgb(best[\"params\"])\n",
    "best_xgb.fit(Xtr_all, y_tr)\n",
    "y_val_pred_xgb = best_xgb.predict(Xval_all)\n",
    "xgb_mae  = mean_absolute_error(y_va, y_val_pred_xgb)\n",
    "xgb_rmse = mean_squared_error(y_va, y_val_pred_xgb, squared=False)\n",
    "xgb_r2   = r2_score(y_va, y_val_pred_xgb)\n",
    "\n",
    "print(\"\\n=== Bestes XGB (ohne Log-Target) ===\")\n",
    "print(best[\"params\"])\n",
    "print(f\"Val: R²={xgb_r2:.4f} | RMSE={xgb_rmse:.1f} | MAE={xgb_mae:.2f}\")\n",
    "\n",
    "# ---------- 4) Zweites Modell: HistGradientBoosting (kleine Suche) ----------\n",
    "hgb_space = [\n",
    "    dict(max_depth=None, max_leaf_nodes=31,  min_samples_leaf=20, l2_regularization=0.0),\n",
    "    dict(max_depth=None, max_leaf_nodes=63,  min_samples_leaf=20, l2_regularization=0.0),\n",
    "    dict(max_depth=None, max_leaf_nodes=127, min_samples_leaf=10, l2_regularization=0.0),\n",
    "    dict(max_depth=6,    max_leaf_nodes=None, min_samples_leaf=20, l2_regularization=0.0),\n",
    "    dict(max_depth=7,    max_leaf_nodes=None, min_samples_leaf=15, l2_regularization=0.1),\n",
    "]\n",
    "best_hgb = None; best_hgb_mae = np.inf; best_hgb_params = None\n",
    "\n",
    "print(\"\\n[INFO] Tune HistGB (kleiner Grid, MAE)…\")\n",
    "for i, hp in enumerate(hgb_space, 1):\n",
    "    hgb = HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_iter=800,            # moderat\n",
    "        early_stopping=True,     # sklearn-intern, robust\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        random_state=42,\n",
    "        **hp\n",
    "    )\n",
    "    hgb.fit(Xtr_all, y_tr)\n",
    "    y_val_pred_hgb = hgb.predict(Xval_all)\n",
    "    mae = mean_absolute_error(y_va, y_val_pred_hgb)\n",
    "    print(f\"[{i}/{len(hgb_space)}] HistGB MAE={mae:.2f}  <- {hp}\")\n",
    "    if mae < best_hgb_mae:\n",
    "        best_hgb_mae, best_hgb, best_hgb_params = mae, hgb, hp\n",
    "\n",
    "print(\"\\n=== Bestes HistGB ===\")\n",
    "print(best_hgb_params)\n",
    "print(f\"Val: MAE={best_hgb_mae:.2f}\")\n",
    "\n",
    "# ---------- 5) Gewichts-Blending (XGB & HistGB) ----------\n",
    "# Finde w in [0,1], s.d. MAE auf Val minimal wird: y = w*y_xgb + (1-w)*y_hgb\n",
    "y_hgb = best_hgb.predict(Xval_all)\n",
    "y_xgb = y_val_pred_xgb\n",
    "\n",
    "ws = np.linspace(0.0, 1.0, 201)\n",
    "maes = [mean_absolute_error(y_va, w*y_xgb + (1-w)*y_hgb) for w in ws]\n",
    "best_idx = int(np.argmin(maes))\n",
    "best_w   = float(ws[best_idx])\n",
    "blend_pred = best_w*y_xgb + (1-best_w)*y_hgb\n",
    "blend_mae  = maes[best_idx]\n",
    "blend_rmse = mean_squared_error(y_va, blend_pred, squared=False)\n",
    "blend_r2   = r2_score(y_va, blend_pred)\n",
    "\n",
    "print(\"\\n=== Blend(XGB, HistGB) ===\")\n",
    "print(f\"Bestes w (XGB-Anteil) = {best_w:.3f}\")\n",
    "print(f\"Val: R²={blend_r2:.4f} | RMSE={blend_rmse:.1f} | MAE={blend_mae:.2f}\")\n",
    "\n",
    "# Optional: Modelle & Blend-Komponenten verfügbar:\n",
    "# - best_xgb (XGBRegressor, non-log)\n",
    "# - best_hgb (HistGradientBoostingRegressor)\n",
    "# - best_w  (Blendgewicht)\n",
    "# - Predictions: y_xgb, y_hgb, blend_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a7bcfe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top groups by ΔMAE (permutation):\n",
      "      group  n_cols  delta_mae_mean  delta_mae_std  baseline_mae   \n",
      "0     cat79       3        0.007031       0.000242     3034.3721  \\\n",
      "1   cat53_B       1        0.005338       0.000196     3034.3721   \n",
      "2    cat1_B       1        0.003771       0.000238     3034.3721   \n",
      "3   cat26_B       1        0.001911       0.000066     3034.3721   \n",
      "4   cat44_B       1        0.001641       0.000109     3034.3721   \n",
      "5    cat100      12        0.001613       0.000230     3034.3721   \n",
      "6    cat112      21        0.001507       0.000082     3034.3721   \n",
      "7     cont3       1        0.001311       0.000083     3034.3721   \n",
      "8    cont14       1        0.001263       0.000132     3034.3721   \n",
      "9     cont1       1        0.001091       0.000049     3034.3721   \n",
      "10    cat94       3        0.000950       0.000064     3034.3721   \n",
      "11    cat73       2        0.000919       0.000030     3034.3721   \n",
      "12  cat49_B       1        0.000839       0.000053     3034.3721   \n",
      "13   cat108       9        0.000535       0.000085     3034.3721   \n",
      "14  cat52_B       1        0.000508       0.000043     3034.3721   \n",
      "15    cont9       1        0.000425       0.000106     3034.3721   \n",
      "16    cat92       2        0.000359       0.000052     3034.3721   \n",
      "17    cat98       4        0.000317       0.000031     3034.3721   \n",
      "18    cat82       3        0.000314       0.000086     3034.3721   \n",
      "19   cat105       6        0.000237       0.000049     3034.3721   \n",
      "\n",
      "    mae_perm_mean  \n",
      "0     3034.379131  \n",
      "1     3034.377438  \n",
      "2     3034.375871  \n",
      "3     3034.374011  \n",
      "4     3034.373741  \n",
      "5     3034.373713  \n",
      "6     3034.373607  \n",
      "7     3034.373411  \n",
      "8     3034.373363  \n",
      "9     3034.373191  \n",
      "10    3034.373050  \n",
      "11    3034.373019  \n",
      "12    3034.372939  \n",
      "13    3034.372635  \n",
      "14    3034.372608  \n",
      "15    3034.372525  \n",
      "16    3034.372459  \n",
      "17    3034.372417  \n",
      "18    3034.372414  \n",
      "19    3034.372337  \n",
      "\n",
      "=== Error slices for cat79_B (overall MAE = 3034.372) ===\n",
      "  slice  count          mae  lift_vs_overall\n",
      "0   0.0   5564  5555.926256      2521.554156\n",
      "1   1.0  24567  2463.283757      -571.088343\n",
      "\n",
      "=== Error slices for cat79_D (overall MAE = 3034.372) ===\n",
      "  slice  count          mae  lift_vs_overall\n",
      "0   1.0   4227  6351.205777      3316.833677\n",
      "1   0.0  25904  2493.133065      -541.239035\n",
      "\n",
      "=== Error slices for cat79_infrequent_sklearn (overall MAE = 3034.372) ===\n",
      "  slice  count          mae  lift_vs_overall\n",
      "0   1.0    272  4181.016848      1146.644748\n",
      "1   0.0  29859  3023.926761       -10.445339\n",
      "\n",
      "=== Error slices for cat53_B (overall MAE = 3034.372) ===\n",
      "  slice  count          mae  lift_vs_overall\n",
      "0   1.0   2390  3545.082983       510.710883\n",
      "1   0.0  27741  2990.372280       -43.999820\n",
      "\n",
      "=== Error slices for cat1_B (overall MAE = 3034.372) ===\n",
      "  slice  count          mae  lift_vs_overall\n",
      "0   0.0  22676  3406.772975       372.400875\n",
      "1   1.0   7455  1901.634041     -1132.738059\n",
      "\n",
      "=== Error slices for cat26_B (overall MAE = 3034.372) ===\n",
      "  slice  count          mae  lift_vs_overall\n",
      "0   1.0   1830  3454.916754       420.544654\n",
      "1   0.0  28301  3007.178831       -27.193269\n",
      "\n",
      "=== Error slices for cat44_B (overall MAE = 3034.372) ===\n",
      "  slice  count         mae  lift_vs_overall\n",
      "0   1.0   2470  3491.64807        457.27597\n",
      "1   0.0  27661  2993.53946        -40.83264\n"
     ]
    }
   ],
   "source": [
    "# ==== MAE-driven Feature Diagnostics: Permutation Importance + Error Slicing ====\n",
    "# Works with any fitted model having .predict().\n",
    "# Inputs you already have: best_xgb (or best_model), Xval_all, y_val\n",
    "# If your model variable is named differently, just set `model = your_model`.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import re\n",
    "from typing import Dict, List, Optional, Tuple, Iterable\n",
    "\n",
    "# ----------------- 0) Helper: baseline MAE -----------------\n",
    "def baseline_mae(model, X_val: pd.DataFrame, y_val: pd.Series | np.ndarray) -> float:\n",
    "    y_hat = model.predict(X_val)\n",
    "    return float(mean_absolute_error(y_val, y_hat))\n",
    "\n",
    "# ----------------- 1) Build groups (e.g., \"cat20_*\" dummies) -----------------\n",
    "def build_groups_from_prefix(\n",
    "    X: pd.DataFrame, \n",
    "    min_group_size: int = 2, \n",
    "    pattern: str = r\"^([^_]+)_\"\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Group columns by the text before the first underscore.\n",
    "    E.g. cat20_A, cat20_B -> group 'cat20'\n",
    "    Columns without '_' or groups of size 1 become singletons.\n",
    "    \"\"\"\n",
    "    groups: Dict[str, List[str]] = {}\n",
    "    for c in X.columns:\n",
    "        m = re.match(pattern, c)\n",
    "        if m:\n",
    "            g = m.group(1)\n",
    "            groups.setdefault(g, []).append(c)\n",
    "        else:\n",
    "            groups.setdefault(c, []).append(c)\n",
    "    # promote groups of size 1 to singleton feature names\n",
    "    final = {}\n",
    "    for g, cols in groups.items():\n",
    "        if len(cols) >= min_group_size:\n",
    "            final[g] = cols\n",
    "        else:\n",
    "            # treat each as its own group (singleton)\n",
    "            for c in cols:\n",
    "                final[c] = [c]\n",
    "    return final\n",
    "\n",
    "# ----------------- 2) Permutation Importance for MAE -----------------\n",
    "def perm_importance_mae(\n",
    "    model,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series | np.ndarray,\n",
    "    groups: Optional[Dict[str, List[str]]] = None,\n",
    "    n_repeats: int = 10,\n",
    "    random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame sorted by mean ΔMAE (higher = more important).\n",
    "    If `groups` is provided, it permutes columns group-wise (recommended for one-hot blocks).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    base = baseline_mae(model, X_val, y_val)\n",
    "    results = []\n",
    "\n",
    "    if groups is None:\n",
    "        groups = {c: [c] for c in X_val.columns}\n",
    "\n",
    "    for name, cols in groups.items():\n",
    "        deltas = []\n",
    "        for _ in range(n_repeats):\n",
    "            Xp = X_val.copy()\n",
    "            # permute each column within the group independently\n",
    "            for c in cols:\n",
    "                Xp[c] = Xp[c].to_numpy()[rng.permutation(len(Xp))]\n",
    "            y_hat = model.predict(Xp)\n",
    "            mae_perm = mean_absolute_error(y_val, y_hat)\n",
    "            deltas.append(mae_perm - base)\n",
    "        results.append({\n",
    "            \"group\": name,\n",
    "            \"n_cols\": len(cols),\n",
    "            \"delta_mae_mean\": float(np.mean(deltas)),\n",
    "            \"delta_mae_std\":  float(np.std(deltas, ddof=1)),\n",
    "            \"baseline_mae\": base,\n",
    "            \"mae_perm_mean\": base + float(np.mean(deltas)),\n",
    "        })\n",
    "\n",
    "    df_imp = pd.DataFrame(results).sort_values(\"delta_mae_mean\", ascending=False).reset_index(drop=True)\n",
    "    return df_imp\n",
    "\n",
    "# ----------------- 3) Error Slicing by Feature -----------------\n",
    "def slice_mae_by_feature(\n",
    "    model,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series | np.ndarray,\n",
    "    feature: str,\n",
    "    n_bins: int = 10,\n",
    "    top_k: int = 12\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For numeric feature: MAE by quantile bins.\n",
    "    For categorical (object/bool or low cardinality): MAE by top levels (+ OTHER).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_val).reshape(-1)\n",
    "    y_pred = model.predict(X_val)\n",
    "    overall = float(mean_absolute_error(y_true, y_pred))\n",
    "    x = X_val[feature]\n",
    "\n",
    "    # Decide numeric vs categorical\n",
    "    is_numeric = np.issubdtype(x.dtype, np.number)\n",
    "\n",
    "    rows = []\n",
    "    if is_numeric and x.nunique() > n_bins:\n",
    "        # Quantile bins\n",
    "        try:\n",
    "            bins = pd.qcut(x, q=n_bins, duplicates=\"drop\")\n",
    "        except Exception:\n",
    "            # fallback to uniform bins if qcut fails\n",
    "            bins = pd.cut(x, bins=n_bins)\n",
    "        g = bins\n",
    "        gb = pd.DataFrame({\"bin\": g, \"y_true\": y_true, \"y_pred\": y_pred})\n",
    "        for b, dfb in gb.groupby(\"bin\"):\n",
    "            mae = float(mean_absolute_error(dfb[\"y_true\"], dfb[\"y_pred\"]))\n",
    "            rows.append({\"slice\": str(b), \"count\": len(dfb), \"mae\": mae, \"lift_vs_overall\": mae - overall})\n",
    "    else:\n",
    "        # Treat as categorical (or numeric with few unique values)\n",
    "        vc = x.value_counts(dropna=False)\n",
    "        keep = set(vc.head(top_k).index)\n",
    "        levels = x.where(x.isin(keep), other=\"__OTHER__\")\n",
    "        gb = pd.DataFrame({\"lvl\": levels, \"y_true\": y_true, \"y_pred\": y_pred}).groupby(\"lvl\")\n",
    "        for lvl, dfb in gb:\n",
    "            mae = float(mean_absolute_error(dfb[\"y_true\"], dfb[\"y_pred\"]))\n",
    "            rows.append({\"slice\": str(lvl), \"count\": len(dfb), \"mae\": mae, \"lift_vs_overall\": mae - overall})\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values(\"mae\", ascending=False).reset_index(drop=True)\n",
    "    out.attrs[\"overall_mae\"] = overall\n",
    "    return out\n",
    "\n",
    "# ----------------- 4) HOW TO USE -----------------\n",
    "# Choose your fitted model:\n",
    "# model = best_xgb   # if you trained the non-log XGB\n",
    "# model = best_model # if your variable is named like this\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    # fallback guesses (edit as needed)\n",
    "    try:\n",
    "        model = best_xgb\n",
    "    except NameError:\n",
    "        model = best_model  # may still raise if neither exists\n",
    "\n",
    "# Build groups (recommended when you one-hot encoded categoricals)\n",
    "groups = build_groups_from_prefix(Xval_all, min_group_size=2)\n",
    "\n",
    "# 4a) Permutation importance (MAE):\n",
    "imp_df = perm_importance_mae(model, Xval_all, y_val, groups=groups, n_repeats=8, random_state=42)\n",
    "print(\"\\nTop groups by ΔMAE (permutation):\")\n",
    "print(imp_df.head(20))\n",
    "\n",
    "# 4b) Error slicing for the top 5 offending groups/features:\n",
    "top_feats = imp_df[\"group\"].head(5).tolist()\n",
    "slice_reports = {}\n",
    "for g in top_feats:\n",
    "    # pick a representative column if this is a group; else the feature itself\n",
    "    cols = groups[g]\n",
    "    # If group -> run slicing for each col, or just the group name if it exists as a column.\n",
    "    # We'll do each column for clarity on which levels/bins are problematic:\n",
    "    for c in cols:\n",
    "        rep = slice_mae_by_feature(model, Xval_all, y_val, c, n_bins=10, top_k=12)\n",
    "        slice_reports[c] = rep\n",
    "        print(f\"\\n=== Error slices for {c} (overall MAE = {rep.attrs['overall_mae']:.3f}) ===\")\n",
    "        print(rep.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
